{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Optimisation et Apprentissage\n",
    "\n",
    "L'objectif de ce projet est d'implémenter les différents modèles de programmation linéaire en variables mixtes vus en cours pour les *Support Vector Machine* (SVM) pour la séparation de points en deux dimensions.\n",
    "\n",
    "**Remarque :** Comme vu en cours, nous utiliserons la norme 1 dans la fonction objective pour les différents modèles.\n",
    "\n",
    "## Attendu pour ce projet\n",
    "\n",
    "1. Générer un dataset composé de 20 points 2D aléatoires. Chaque point aura un label parmi $\\{-1,1\\}$ de telle manière que :\n",
    "      * chaque classe a le même nombre de points,\n",
    "      * les points sont linéairement séparables. \n",
    "2. Implémenter le problème d'optimisation linéaire déterminant l'hyperplan optimal et donner l'équation de cet hyperplan.\n",
    "3. Modifier le label de 5 points afin de rendre ce dataset non linéairement séparable.\n",
    "4. Implémenter deux modèles vus en cours (selon les losses choisies : *hinge*, *Ramp* ou *Hard margin*) permettant d'obtenir les hyperplans optimaux. Donner les deux hyperplans obtenus.\n",
    "5. Comparer et discuter les résultats obtenus par les différents modèles.\n",
    "\n",
    "**Optionnel :** \n",
    "* Vous pouvez ajouter des figures (générées avec `mathplotlib` par exemple) pour illustrer les hyperplans obtenus.\n",
    "* Vous pouvez évaluer l'impact de la variation de la pondération $C$ utilisée dans les losses.\n",
    "* Vous pouvez tester sur un dataset plus important.\n",
    "\n",
    "Le projet doit être un fichier notebook python contenant les modèles et les résultats. Des images/photos peuvent être insérées dans ce fichier. \n",
    "Ce dernier doit être envoyé par mail à **lacroix@lipn.fr** avant le **9 mars 2025**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/auduvignac/SVM_optimisation/blob/main/SVM-Optimisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installons préalablement les librairies requises si ce n'est pas déjà le cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipython\n",
    "%pip install matplotlib\n",
    "%pip install mip\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.\n",
    "Générer un dataset composé de 20 points 2D aléatoires. Chaque point aura un label parmi $\\{-1,1\\}$ de telle manière que :\n",
    " - chaque classe a le même nombre de points,\n",
    " - les points sont linéairement séparables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixer la graine aléatoire pour reproductibilité\n",
    "np.random.seed(42)\n",
    "\n",
    "# Générer 10 points pour la classe -1\n",
    "class_neg = np.random.randn(10, 2) - 2  # Décaler pour séparer les classes\n",
    "\n",
    "# Générer 10 points pour la classe 1\n",
    "class_pos = np.random.randn(10, 2) + 2  # Décaler pour séparer les classes\n",
    "\n",
    "# Ajouter les labels\n",
    "labels_neg = -np.ones(10)\n",
    "labels_pos = np.ones(10)\n",
    "\n",
    "# Combiner les points et les labels\n",
    "points = np.vstack((class_neg, class_pos))\n",
    "labels = np.hstack((labels_neg, labels_pos))\n",
    "\n",
    "# Mélanger les points et les labels\n",
    "indices = np.arange(20)\n",
    "np.random.shuffle(indices)\n",
    "points = points[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# Afficher les points\n",
    "plt.plot(\n",
    "    points[:, 0][labels == -1], points[:, 1][labels == -1], \"bo\", label=\"-1\"\n",
    ")  # Cercles bleus pour -1\n",
    "plt.plot(\n",
    "    points[:, 0][labels == 1], points[:, 1][labels == 1], \"ro\", label=\"+1\"\n",
    ")  # Cercles rouges pour +1\n",
    "plt.title(\"Dataset de 20 points 2D linéairement séparables\")\n",
    "plt.xlabel(r\"$x_{1}$\")\n",
    "plt.ylabel(r\"$x_{2}$\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.\n",
    "Implémenter le problème d'optimisation linéaire déterminant l'hyperplan optimal et donner l'équation de cet hyperplan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappels concernant l'expression de la distance minimale dans le cadre de la formulation du problème d'optimisation de la SVM\n",
    "\n",
    "Le schéma ci-dessous illustre la formulation du problème d'optimisation de la SVM entraînée sur deux classes. Les échantillons situés sur les marges sont appelés vecteurs de support.\n",
    "Plusieurs éléments apparaissent sur le schéma :\n",
    "- les marges en bleu et rouges qui sont les droites parallèles à l'hyperplan optimal et passant par les vecteurs de support;\n",
    "- l'hyperplan optimal permettant de séparer les deux classes en rouge, dont l'expression est $w^{T} x + b = 0$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/auduvignac/SVM_optimisation/refs/heads/main/Figures/rapport/SVM_schema.png\">\n",
    "\n",
    "Par définition :\n",
    "- Soit $H$ l'hyperplan optimal séparant les deux classes. Pour tout point $x_{H}$ appartenant à cet hyperplan et $w$ un vecteur orthogonal à $H$, nous avons $w^{T} x_{H} + b = 0$ ;\n",
    "- Soit $x_{+}$ (respectivement $x_{-}$) les vecteurs appartenant à la classe $+1$ (respectivement $-1$), nous avons : $w^{T} x_{+} + b \\geq +1$ et $w^{T} x_{-} + b \\leq -1$ ;\n",
    "- Soit $y_{i} \\; \\forall i \\in [1; n]$, les labels associés aux vecteurs $x_{i}$, nous avons : $y_{i} (w^{T} x_{i} + b) \\geq +1$.\n",
    "\n",
    "En décomposant un vecteur x selon $x_{H} \\in H$ et le vecteur $w$, nous avons : $x = x_{H} + d(x, H) \\cdot w$, ce qui équivaut à dire que $x_{H} = x - d(x, H) \\cdot w$.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/auduvignac/SVM_optimisation/refs/heads/main/Figures/rapport/SVM_schema_projection_x.png\">\n",
    "\n",
    "En injectant cette expression dans l'équation de l'hyperplan, nous obtenons : $w^{T} (x - d(x, H) \\cdot w) + b = 0$.\n",
    "\n",
    "Ainsi, nous avons : $w^{T} x - d(x, H) \\cdot w^{T} w + b = 0$.\n",
    "\n",
    "En notant $w^{T} w = \\|w\\|_{L_{k}}$ pour $k \\in \\{1, 2\\}$, nous obtenons : $d(x, H) = \\frac{w^{T} x + b}{\\|w\\|_{L_{k}}}$.\n",
    "\n",
    "Ici, $L_{k}$ représente la norme $k$ pour $k \\in \\{1, 2\\}$ :\n",
    "- $L_{1}$ correspond à la norme 1 qui s'exprime comme $\\|w\\|_{L_{1}} = \\sum\\limits_{i=1}^{n} |w_{j}|$ ;\n",
    "- $L_{2}$ correspond à la norme 2 qui s'exprime comme $\\|w\\|_{L_{2}} = \\sum\\limits_{i=1}^{n} w_{j}^{2}$.\n",
    "\n",
    "Dans le cadre de ce projet nous utiliserons la norme 1 pour une formulation MILP (Mixed Integer Linear Programming : Optimisation linéaire en nombres entiers) dont la forme canonique est la suivante :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{x}}{\\text{minimiser}}\n",
    "& & \\mathbf{c}^{T} \\mathbf{x} \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & A \\mathbf{x} \\leq \\mathbf{b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "L'objectif de la SVM est de maximiser la marge entre les deux classes, c'est à dire maximiser la distance minimale entre les points et l'hyperplan optimal. Cela peut s'exprimer comme suit :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{maximiser}}\n",
    "& & \\frac{2}{\\|w\\|_{L_{1}}} \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & y_{i} (w^{T} x_{i} + b) \\geq 1, \\quad \\forall i \\in [1; n]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Ce qui en revient à minimiser $\\frac{1}{2} \\|w\\|_{L_{1}}$ sous les contraintes $y_{i} (w^{T} x_{i} + b) \\geq 1, \\quad \\forall i \\in [1; n]$.\n",
    "\n",
    "Dans la suite, pour simplifier l'écriture, nous noterons la norme $k$ de $w$ par $\\|w\\|_{k}$ pour $k \\in \\{1, 2\\}$.\n",
    "\n",
    "### Rappels concernant la formulation du problème d'optimisation de la SVM à marge rigide classique et reformulation.\n",
    "\n",
    "Nous cherchons à résoudre le problème d'optimisation suivant :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{w} \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\text{minimiser}}\n",
    "& & \\frac{1}{2} \\|\\mathbf{w}\\|_{k} \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i \\in \\{1, \\ldots, n\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "En l'occurrence $p = 3$, $n = 20$ et $k=1$ (*i.e.* la norme 1 qui s'exprime comme $\\|\\mathbf{w}\\|_{1} = \\sum\\limits_{j=1}^{p} |w_j|$).\n",
    "\n",
    "De plus, étant donné que les points $x_{i}$ s'exprime par l'intermédiaire de deux coordonnées que nous noterons $x_{1, i}$ et $x_{2, i}$ associées respectivement à l'abscisse et à l'ordonnée de $x_{i}$ et que $w = [w_{1}, w_{2}]$, nous avons : $w^{T} \\mathbf{x}_i = w_{1} x_{1, i} + w_{2} x_{2, i}$.\n",
    "Ainsi, nous pouvons réécrire le problème précédent comme suit :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{w}, b}{\\text{minimiser}}\n",
    "& & \\frac{1}{2} \\| \\mathbf{w} \\|_1 \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & y_i (w_{1} x_{1, i} + w_{2} x_{2, i} + b) \\geq 1, \\quad \\forall i \\in \\{1, \\ldots, n\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Pour une cohérence d'écriture, nous noterons le biais $b$ par $w_{3}$ et $x_{i, 3} = 1$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{w}}{\\text{minimiser}}\n",
    "& & \\frac{1}{2} | \\mathbf{w}_{1} | + \\frac{1}{2} | \\mathbf{w}_{2} | + 0 | \\mathbf{w}_{3} | \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & y_i \\sum_{j=1}^{p=3} w_{j} x_{i, j} \\geq 1, \\quad \\forall i \\in \\{1, \\ldots, n\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "En modifiant les contraintes et en reformulant le problème d'optimisation de la SVM à marge rigide, nous obtenons :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{w}}{\\text{minimiser}}\n",
    "& & \\frac{1}{2} | \\mathbf{w}_{1} | + \\frac{1}{2} | \\mathbf{w}_{2} | + 0 | \\mathbf{w}_{3} | \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & -y_i \\sum_{j=1}^{p=3} w_{j} x_{i, j} \\leq - 1, \\quad \\forall i \\in \\{1, \\ldots, n\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "avec $\\forall i \\in \\{1, \\ldots, n\\}, x_{i, 3} = 1$\n",
    "\n",
    "\n",
    "Par ailleurs, la forme canonique d'un problème d'optimisation linéaire, formulée dans le cours, est la suivante :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{x}}{\\text{minimiser}}\n",
    "& & \\mathbf{c}^{T} \\mathbf{x} \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & A \\mathbf{x} \\leq \\mathbf{b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "En l'occurrence $x = w$. Ainsi en associant le problème d'optimisation de la SVM à marge rigide à la forme canonique d'un problème d'optimisation linéaire, nous obtenons :\n",
    "\n",
    "$$\n",
    "x = w = \\begin{bmatrix} w_{1} \\\\ w_{2} \\\\ w_{3} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c = \\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = \\begin{bmatrix} -1 \\\\ -1 \\\\ \\vdots \\\\ -1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} -y_{1} x_{1, 1} & -y_{1} x_{1, 2} & -y_{1} x_{1, 3} \\\\ -y_{2} x_{2, 1} & -y_{2} x_{2, 2} & -y_{2} x_{2, 3} \\\\ \\vdots & \\vdots & \\vdots \\\\ -y_{n} x_{n, 1} & -y_{n} x_{n, 2} & -y_{n} x_{n, 3} \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prélablement, nous allons créer une classe abstraite `SVMBase` qui ui va contenir les méthodes et attributs communs aux différents types de SVM. Cette classe abstraite ne sera pas instanciable directement, mais servira de base pour les classes concrètes qui implémenteront les différents types de SVM.\n",
    "\n",
    "Cette classe contiendra les méthode suivantes :\n",
    "- `__init__` : Initialisation des points et labels.\n",
    "- `set_model()` : Méthode abstraite pour définir les matrices spécifiques (chaque classe fille implémentera la sienne).\n",
    "- `show_model()` : Affiche les détails du modèle (fonction objectif, variables et contraintes) ;\n",
    "- `solve()` : Optimisation du modèle ;\n",
    "- `train_model()` : Enchaîne set_model() et solve() ;\n",
    "- `get_hyperplane_equation()` : Retourne l’équation de l’hyperplan ;\n",
    "- `plot_results()` : Affiche les résultats (commune à toutes les variantes) ;\n",
    "- `run_and_plot()` : Entraîne le modèle, affiche l'équation de l'hyperplan et trace les résultats ;\n",
    "- `compute_errors()` : Calcule les erreurs de classification en fonction des étiquettes réelles et du vecteur $w$ :\n",
    "\t1. Calcul du  nombre de points mal classés : $\\sum\\limits_{i=1}^{n} \\mathbb{1}_{y_i \\times (w_{1} x_{i, 1} + w_{2} x_{i, 2} + w_{3}) \\; < \\; 0}$ ;\n",
    "\t2. Calcul du taux d'erreur :$\\frac{\\sum\\limits_{i=1}^{n} \\mathbb{1}_{y_i \\times (w_{1} x_{i, 1} + w_{2} x_{i, 2} + w_{3}) \\; < \\; 0}}{n} \\times 100$\n",
    "- `train_multiple_models` : Entraîne plusieurs modèles SVM avec différentes valeurs de C et retourne leurs résultats ;\n",
    "- `detect_vectors` : Détection des vecteurs de support ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from IPython.display import display, Math\n",
    "from matplotlib import cm\n",
    "from mip import Model, xsum, minimize, CONTINUOUS, BINARY\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class SVMBase(ABC):\n",
    "    \"\"\"\n",
    "    Classe abstraite pour implémenter différentes variantes du SVM avec MIP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, points, labels):\n",
    "        \"\"\"\n",
    "        Initialise les données du problème.\n",
    "\n",
    "        :param points: (np.array) Tableau des points d'entrée (X, Y).\n",
    "        :param labels: (np.array) Tableau des étiquettes (-1 ou 1).\n",
    "        :param C: (float) Paramètre de régularisation (utile pour Hinge Loss).\n",
    "        \"\"\"\n",
    "        self.points = points[:]\n",
    "        self.labels = labels[:]\n",
    "        self.n = len(points)  # Nombre de points\n",
    "        self.model = None\n",
    "        self.w = None  # Coefficients de l'hyperplan\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_model(self):\n",
    "        \"\"\"\n",
    "        Méthode abstraite pour définir le modèle spécifique à chaque SVM.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def show_model(self):\n",
    "        \"\"\"\n",
    "        Affiche les détails du modèle (fonction objectif, variables et contraintes).\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "          print(\"Veuillez d'abord définir le modèle avec `set_model()`.\")\n",
    "          return\n",
    "        output = []\n",
    "        model = self.model\n",
    "\n",
    "        # Fonction objectif\n",
    "        output.append(\"\\n Fonction objectif :\")\n",
    "        output.append(str(self.model.objective))\n",
    "\n",
    "        # Variables\n",
    "        output.append(\"\\n Variables du modèle :\")\n",
    "        for var in model.vars:\n",
    "          output.append(f\"{var.name}\")\n",
    "\n",
    "        # Contraintes\n",
    "        output.append(\"\\n Contraintes du modèle :\")\n",
    "        for constr in model.constrs:\n",
    "          output.append(str(constr))\n",
    "\n",
    "        print(\"\\n\".join(output))\n",
    "\n",
    "\n",
    "    def solve(self, verbose=False):\n",
    "        \"\"\"\n",
    "        Résout le modèle MIP.\n",
    "\n",
    "        :param verbose: (bool) Activer ou non les logs de l'optimisation.\n",
    "        \"\"\"\n",
    "        if not verbose:\n",
    "            self.model.verbose = 0\n",
    "        self.model.optimize()\n",
    "\n",
    "        self.w = [v.x for v in self.model.vars[:3]]\n",
    "\n",
    "    def train_model(self, verbose=False):\n",
    "        \"\"\"\n",
    "        Entraîne le modèle en appelant `set_model()` puis `solve()`.\n",
    "\n",
    "        :param verbose: (bool) Activer ou non les logs de l'optimisation.\n",
    "        \"\"\"\n",
    "        self.set_model()\n",
    "        self.solve(verbose)\n",
    "\n",
    "    def get_hyperplane_equation(self):\n",
    "        \"\"\"\n",
    "        Retourne l'équation de l'hyperplan sous forme de chaîne de caractères.\n",
    "        \"\"\"\n",
    "        if self.w is None:\n",
    "            return \"Le modèle n'a pas été entraîné.\"\n",
    "        return rf\"$\\text{{Equation de l'hyperplan: }} {self.w[0]:.2f}x_1 + {self.w[1]:.2f}x_2 + {self.w[2]:.2f} = 0$\"\n",
    "\n",
    "    def plot_results(self, plot_marge=False, plot_support_vectors=False):\n",
    "        \"\"\"\n",
    "        Affiche les résultats de la classification avec l'hyperplan et les marges.\n",
    "\n",
    "        :param plot_marge: (bool) Afficher ou non les marges (+1 et -1).\n",
    "        \"\"\"\n",
    "        if self.w is None:\n",
    "            print(\n",
    "                \"Veuillez entraîner le modèle avant d'afficher les résultats.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Tracer les points\n",
    "        plt.plot(\n",
    "          points[:, 0][labels == -1], points[:, 1][labels == -1], \"bo\", label=\"-1\"\n",
    "        )  # Cercles bleus pour -1\n",
    "        plt.plot(\n",
    "          points[:, 0][labels == 1], points[:, 1][labels == 1], \"ro\", label=\"+1\"\n",
    "        )  # Cercles rouges pour +1\n",
    "\n",
    "        # Tracer l'hyperplan\n",
    "        x_min, x_max = self.points[:, 0].min() - 1, self.points[:, 0].max() + 1\n",
    "        y_min, y_max = self.points[:, 1].min() - 1, self.points[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500)\n",
    "        )\n",
    "        Z = self.w[0] * xx + self.w[1] * yy + self.w[2]\n",
    "        plt.contour(xx, yy, Z, levels=[0], colors=\"k\", linestyles=\"dashed\")\n",
    "\n",
    "        if plot_marge:\n",
    "            # Tracer les marges (+1 et -1)\n",
    "            plt.contour(\n",
    "                xx,\n",
    "                yy,\n",
    "                Z,\n",
    "                levels=sorted([-1, 1]),\n",
    "                colors=\"k\",\n",
    "                linestyles=\"dotted\",\n",
    "            )\n",
    "        if plot_support_vectors:\n",
    "            # Détection des vecteurs supports\n",
    "            support_vectors = self.detect_vectors()\n",
    "            # extraction des index de chaque vecteur support\n",
    "            support_vectors = [v.get(\"index\") for v in support_vectors]\n",
    "\n",
    "            plt.scatter(\n",
    "              self.points[support_vectors, 0],\n",
    "              self.points[support_vectors, 1],\n",
    "              s=100,\n",
    "              facecolors=\"none\",\n",
    "              edgecolors=\"yellow\",\n",
    "              linewidths=2,\n",
    "              label=\"Vecteurs supports\",\n",
    "              zorder=3,  # S'assure que les points sont bien affichés au premier plan\n",
    "            )\n",
    "\n",
    "        plt.title(\"Hyperplan optimal séparant les deux classes\")\n",
    "        plt.xlabel(r\"$x_{1}$\")\n",
    "        plt.ylabel(r\"$x_{2}$\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    def run_and_plot(self, plot_marge=False, plot_support_vectors= False, verbose=False):\n",
    "        \"\"\"\n",
    "        Entraîne le modèle, affiche l'équation de l'hyperplan et trace les résultats.\n",
    "\n",
    "        :param plot_marge: (bool) Afficher ou non les marges (+1 et -1).\n",
    "        :param verbose: (bool) Activer ou non les logs de l'optimisation.\n",
    "        \"\"\"\n",
    "        self.train_model(verbose)\n",
    "        equation = self.get_hyperplane_equation()\n",
    "        display(Math(equation))\n",
    "        self.plot_results(\n",
    "            plot_marge=plot_marge, plot_support_vectors=plot_support_vectors\n",
    "        )\n",
    "\n",
    "    def compute_errors(self):\n",
    "        \"\"\"\n",
    "        Calcule les erreurs de classification.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.w is None:\n",
    "            return \"Erreur : veuillez d'abord entraîner le modèle avec `train_model()`.\"\n",
    "\n",
    "        # Vérification des points mal classés\n",
    "        misclassified_points = sum(\n",
    "            self.labels[i]\n",
    "            * (\n",
    "                self.w[0] * self.points[i][0]\n",
    "                + self.w[1] * self.points[i][1]\n",
    "                + self.w[2]\n",
    "            )\n",
    "            < 0\n",
    "            for i in range(self.n)\n",
    "        )\n",
    "\n",
    "        error_rate = (\n",
    "            misclassified_points / self.n\n",
    "        ) * 100  # Taux d'erreur en %\n",
    "\n",
    "        self.errors = {\n",
    "            \"misclassified points\": misclassified_points,\n",
    "            \"error rate\": error_rate,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def train_multiple_models(cls, points, labels, C_values):\n",
    "        \"\"\"\n",
    "        Entraîne plusieurs modèles SVM avec différentes valeurs de C et retourne leurs résultats.\n",
    "\n",
    "        :param points: Données d'entrée.\n",
    "        :param labels: Étiquettes associées.\n",
    "        :param C_values: Liste des valeurs de C à tester.\n",
    "        :return: Liste des modèles entraînés et liste des erreurs associées.\n",
    "        \"\"\"\n",
    "        trained_models = []\n",
    "        errors = []\n",
    "\n",
    "        for C in C_values:\n",
    "            current_svm = cls(points, labels, C)\n",
    "            current_svm.train_model()\n",
    "            current_svm.compute_errors()\n",
    "            trained_models.append(current_svm)\n",
    "            errors.append(current_svm.errors)\n",
    "\n",
    "        return (\n",
    "            trained_models,\n",
    "            errors,\n",
    "        )  # On retourne les modèles et leurs erreurs\n",
    "\n",
    "    def detect_vectors(self):\n",
    "        \"\"\"\n",
    "        Détecte les vecteurs de support en affichant les marges.\n",
    "        \"\"\"\n",
    "        if self.w is None:\n",
    "            print(\n",
    "                \"Erreur : veuillez d'abord entraîner le modèle avec `train_model()`.\"\n",
    "            )\n",
    "            return\n",
    "        vectors = []\n",
    "        for i in range(self.n):\n",
    "            margin_value = self.labels[i] * (\n",
    "                self.w[0] * self.points[i][0]\n",
    "                + self.w[1] * self.points[i][1]\n",
    "                + self.w[2]\n",
    "            )\n",
    "\n",
    "            if np.isclose(margin_value, 1, atol=1e-3):  # Vecteur support\n",
    "                vectors.append({\"index\": i, \"v\" : self.points[i]})\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ensuite implémenter la classe `SVMHardMarginClassical` qui hérite de la classe `SVMBase` et qui implémente les méthodes abstraites de cette dernière. Cette classe permettra de résoudre le problème d'optimisation de la SVM à marge rigide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMHardMarginClassical(SVMBase):\n",
    "    \"\"\"\n",
    "    Implémentation du SVM à marge rigide (Hard Margin SVM) avec MIP.\n",
    "    \"\"\"\n",
    "    def set_model(self):\n",
    "        \"\"\"\n",
    "        Crée le modèle MIP pour le SVM à marge rigide.\n",
    "        \"\"\"\n",
    "        m = 3  # Nombre de variables (w1, w2, w3)\n",
    "        c = [0.5, 0.5, 0]  # Vecteur des coûts\n",
    "        b = [-1 for _ in range(self.n)]  # Vecteur des contraintes\n",
    "        A = [\n",
    "            [\n",
    "                -self.labels[i] * self.points[i][0],\n",
    "                -self.labels[i] * self.points[i][1],\n",
    "                -self.labels[i],\n",
    "            ]\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "        # Création du modèle\n",
    "        self.model = Model(\"HardMarginClassical\")\n",
    "        w_vars = [\n",
    "            self.model.add_var(var_type=CONTINUOUS, name=f\"w_{j+1}\")\n",
    "            for j in range(m)\n",
    "        ]\n",
    "\n",
    "        # Ajout de la fonction objectif\n",
    "        self.model.objective = minimize(\n",
    "            xsum(c[i] * w_vars[i] for i in range(m))\n",
    "        )\n",
    "\n",
    "        # Ajout des contraintes\n",
    "        for i in range(self.n):\n",
    "            self.model += xsum(A[i][j] * w_vars[j] for j in range(m)) <= b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMhardmarginclassical = SVMHardMarginClassical(points, labels)\n",
    "SVMhardmarginclassical.set_model()  # Définit le modèle\n",
    "SVMhardmarginclassical.model.write(\"model.lp\")  # Sauvegarde le modèle\n",
    "SVMhardmarginclassical.show_model()  # Affiche le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat ci-dessus présente le modèle tel que défini dans la méthode `set_model()` de la classe `SVMHardMarginClassical`.\n",
    "Il est en adéquation avec la formulation précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMhardmarginclassical.run_and_plot()\n",
    "SVMhardmarginclassical.compute_errors()\n",
    "print(f\"Taux d'erreurs : {SVMhardmarginclassical.errors.get('error rate')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme en témoigne la figure ci-dessous, avec le dataset généré, et l'yperplan optimal obtenu avec la SVM à marge rigide : $0.52x + 0.64y + 0.00 = 0$, nous obtenons un taux d'erreur de 0%.\n",
    "\n",
    "Pour étoffer le propos, affichons les frontières de décision ainsi que les vecteurs supports par l'intermédiaire de la méthode `plot_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMhardmarginclassical.plot_results(plot_marge=True, plot_support_vectors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3.\n",
    "Modifier le label de 5 points afin de rendre ce dataset non linéairement séparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela se réalisera en trois temps :\n",
    "1. **Sélection des Indices** : Nous utilisons `np.random.choice` pour sélectionner 5 indices aléatoires parmi les 20 points.\n",
    "2. **Inversion des Labels** : Nous inversons les labels des points sélectionnés en multipliant par -1.\n",
    "3. **Affichage et Visualisation** : Nous affichons les nouveaux labels et visualisons les points avec leurs nouveaux labels pour vérifier que le dataset est maintenant non linéairement séparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sélectionner 5 indices aléatoires\n",
    "indices_to_flip = np.random.choice(20, 5, replace=False)\n",
    "\n",
    "# Inverser les labels des points sélectionnés\n",
    "labels[indices_to_flip] = -labels[indices_to_flip]\n",
    "\n",
    "# Visualiser les points avec les nouveaux labels\n",
    "# Afficher les points\n",
    "plt.plot(\n",
    "    points[:, 0][labels == -1], points[:, 1][labels == -1], \"bo\", label=\"-1\"\n",
    ")  # Cercles bleus pour -1\n",
    "plt.plot(\n",
    "    points[:, 0][labels == 1], points[:, 1][labels == 1], \"ro\", label=\"+1\"\n",
    ")  # Cercles rouges pour +1\n",
    "plt.title(\"Dataset de 20 points 2D non linéairement séparables\")\n",
    "plt.xlabel(r\"$x_{1}$\")\n",
    "plt.ylabel(r\"$x_{2}$\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.\n",
    "Implémenter deux modèles vus en cours (selon les losses choisies : hinge, Ramp ou Hard margin) permettant d'obtenir les hyperplans optimaux. Donner les deux hyperplans obtenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation du problème d'optimisation de la SVM avec *Hinge Loss* et reformulation.\n",
    "\n",
    "Nous cherchons cherche à résoudre le problème d'optimisation suivant :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{w} \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\text{minimiser}}\n",
    "& & \\frac{1}{2} \\|\\mathbf{w}\\| + C \\sum\\limits_{i=1}^{n} \\epsilon_{i} \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\epsilon_{i}, \\quad \\forall i \\in \\{1, \\ldots, n\\} \\\\\n",
    "& & & \\epsilon_{i} \\geq 0, \\quad \\forall i \\in \\{1, \\ldots, n\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Avec, pour rappel, $p = 3$ et $n = 20$ et $\\|\\mathbf{w}\\| = \\sum\\limits_{j=1}^{p} |w_j|$ et $\\epsilon_{i}$ l'erreur de $x_{i}$ si ce dernier est dans la marge :\n",
    "$$\n",
    "\\epsilon_{i} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "0 & \\text{si } y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 \\text{ ou } y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\leq -1 \\\\\n",
    "1 - y_i (\\mathbf{w}^T \\mathbf{x}_i + b) & \\text{sinon}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Si $y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\leq -1$ alors on a une erreur de $2$ quelle que soit la valeur $y_i (\\mathbf{w}^T \\mathbf{x}_i + b)$\n",
    "\n",
    "Dans un démarche analogue à celle réalisée dans le cadre de la reformulation de la SVM à marge rigide, nous pouvons reformuler le problème ci-dessus de la manière suivante (nous poserons de nouveau $b = w_3$):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{(\\mathbf{w_0}, \\mathbf{w_1}, \\mathbf{w_2}) \\in \\mathbb{R}^{3}, \\epsilon_{i} \\in \\mathbb{R}^{+}}{\\text{minimiser}}\n",
    "& & \\frac{1}{2} | \\mathbf{w}_{1} | + \\frac{1}{2} | \\mathbf{w}_{2} | + 0 | \\mathbf{w}_{3} | + C \\sum\\limits_{i=1}^{n} \\epsilon_{i}  \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & y_i (w_{1} x_{1, i} + w_{2} x_{2, i} + w_{3}) \\geq 1 - \\epsilon_{i}, \\quad \\forall i \\in \\{1, \\ldots, n\\} \\\\\n",
    "& & & \\epsilon_{i} \\geq 0, \\quad \\forall i \\in \\{1, \\ldots, n\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "En modifiant les contraintes et en reformulant le problème d'optimisation de la SVM avec Hinge Loss, nous obtenons :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{(\\mathbf{w_0}, \\mathbf{w_1}, \\mathbf{w_2}) \\in \\mathbb{R}^{3}, \\epsilon_{i} \\in \\mathbb{R}}{\\text{minimiser}}\n",
    "& & \\frac{1}{2} | \\mathbf{w}_{1} | + \\frac{1}{2} | \\mathbf{w}_{2} | + 0 | \\mathbf{w}_{3} | + C \\sum\\limits_{i=1}^{n} \\epsilon_{i}  \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & -y_i (w_{1} x_{1, i} + w_{2} x_{2, i} + w_{3}) - \\epsilon_{i} \\leq - 1, \\quad \\forall i \\in \\{1, \\ldots, n\\} \\\\\n",
    "& & & \\epsilon_{i} \\geq 0, \\quad \\forall i \\in \\{1, \\ldots, n\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Nous cherchons à exprimer ce problème sous la forme canonique d'un problème d'optimisation linéaire :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{x}}{\\text{minimiser}}\n",
    "& & \\mathbf{c}^{T} \\mathbf{x} \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & A \\mathbf{x} \\leq \\mathbf{b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "avec $\\mathbf{x} = \\begin{bmatrix} \\mathbf{w} \\\\ \\boldsymbol{\\epsilon} \\end{bmatrix}$, et les matrices définies comme suit :\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} w_{1} \\\\ w_{2} \\\\ w_{3} \\\\ \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n",
    "$$\n",
    "avec $\\mathbf{x} \\in \\mathbb{R}^{n+3}$\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = \\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\\\ C \\\\ C \\\\ \\vdots \\\\ C \\end{bmatrix}\n",
    "$$\n",
    "avec $\\mathbf{c} \\in \\mathbb{R}^{n+3}$\n",
    "\n",
    "$$\n",
    "\\mathbf{b} = \\begin{bmatrix} -1 \\\\ -1 \\\\ \\vdots \\\\ -1 \\end{bmatrix}\n",
    "$$\n",
    "avec $\\mathbf{b} \\in \\mathbb{R}^{n}$\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "-y_{1} x_{1,1} & -y_{1} x_{1,2} & -y_{1} & -1 & 0 & \\dots & 0 \\\\\n",
    "-y_{2} x_{2,1} & -y_{2} x_{2,2} & -y_{2} & 0 & -1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-y_{n} x_{n,1} & -y_{n} x_{n,2} & -y_{n} & 0 & 0 & \\dots & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "avec $A \\in \\mathbb{R}^{n \\times (n+3)}$\n",
    "\n",
    "### Elements de précision relatifs à l'interprétation des $\\epsilon_i$ dans le cadre de la SVM avec *Hinge Loss*\n",
    "\n",
    "Les **variables slack** $\\epsilon_i$ sont introduites pour gérer les erreurs de classification et permettre une **marge souple** (*soft margin*).\n",
    "Si un point  $x_i$ est bien classé et hors de la marge** (*i.e.* $y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1$), alors **$\\epsilon_i = 0$** car aucune erreur n'est commise.\n",
    "Si en revanche un point $x_i$ est mal classé ou à l'intérieur de la marge (*i.e.* $y_i (\\mathbf{w}^T \\mathbf{x}_i + b) < 1$), dans ce cas :\n",
    "  - $\\epsilon_i > 0$ indique de combien le point $x_i$ est du mauvais côté de la marge ;\n",
    "  - Les valeurs des $\\epsilon_i$ sont proportionnels à l'erreur commise ;\n",
    "  - Si $\\epsilon_i \\geq 1$, alors le point est totalement mal classé ($i.e.$ il est du mauvais côté de l'hyperplan).\n",
    "\n",
    "Sur la base de ce constat, nous allons sur-définir la méthode `compute_errors()` héritée de la classe `SVMBase` pour y intégrer les $\\epsilon_{i}$.\n",
    "Nous allons ainsi calculer la moyenne des $\\epsilon_{i}$ : $\\frac{1}{n} \\sum\\limits_{i=1}^{n} \\epsilon_{i}$\n",
    "\n",
    "### Elements relatifs à l'implémentation du modèle de la SVM avec *Hinge Loss*\n",
    "\n",
    "Dans la continuité de l'implémentation de la classe `SVMHardMargin` nous allons créer une classe `SVMHingeLoss` qui hérite de la classe `SVMBase` et qui implémente les méthodes abstraites de cette dernière en vue de résoudre le problème d'optimisation de la SVM avec *Hinge Loss*.\n",
    "\n",
    "Dans le code ci-dessous, pour une aisance de lecture dans la définition du modèle à minimiser, nous allons exploiter la fonction de `xsum` de la librairie `mip` de la manière suivante :\n",
    "```python\n",
    "model.objective = minimize(\n",
    "    xsum(c[i] * w_vars[i] for i in range(3)) + C * xsum(epsilon_vars)\n",
    "  )\n",
    "```\n",
    "C'est la raison pour laquelle nious pouvons définir $c$ sans la partie relative à $\\boldsymbol{\\epsilon}$ :\n",
    "```python\n",
    "c = [0.5, 0.5, 0]\n",
    "```\n",
    "\n",
    "Nous aurions pu tout à fait définir la fonction objective ainsi :\n",
    "```python\n",
    "model.objective = minimize(\n",
    "    xsum(c[i] * x_vars[i] for i in range(n+3))\n",
    "  )\n",
    "```\n",
    "avec :\n",
    "```python\n",
    "x_vars = w_vars + epsilon_vars\n",
    "```\n",
    "Mais dans ce cas de figure $\\mathbf{c}$ aurait du s'exprimer ainsi :\n",
    "```python\n",
    "c = [0.5, 0.5, 0] + [C for _ in range(n)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMHingeLoss(SVMBase):\n",
    "    \"\"\"\n",
    "    Implémentation du SVM avec Hinge Loss (marge souple).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, points, labels, C=1):\n",
    "        \"\"\"\n",
    "        Initialise le SVM avec Hinge Loss.\n",
    "        \"\"\"\n",
    "        super().__init__(points, labels)\n",
    "        self.C = C\n",
    "\n",
    "    def set_model(self):\n",
    "        \"\"\"\n",
    "        Crée le modèle MIP pour le SVM avec Hinge Loss.\n",
    "        \"\"\"\n",
    "        m = self.n + 3\n",
    "        c = [0.5, 0.5, 0] + [\n",
    "            self.C for _ in range(self.n)\n",
    "        ]  # Vecteur des coûts\n",
    "        b = [-1 for _ in range(self.n)]  # Vecteur des contraintes\n",
    "\n",
    "        # Matrice des contraintes\n",
    "        A = []\n",
    "        for i in range(self.n):\n",
    "            l = [\n",
    "                -self.labels[i] * self.points[i][0],\n",
    "                -self.labels[i] * self.points[i][1],\n",
    "                -self.labels[i],\n",
    "            ]\n",
    "            l += [-1 if j == i else 0 for j in range(self.n)]\n",
    "            A.append(l)\n",
    "\n",
    "        # Création du modèle\n",
    "        self.model = Model(\"HingeLoss\")\n",
    "\n",
    "        # Création des variables\n",
    "        w_vars = [\n",
    "            self.model.add_var(var_type=CONTINUOUS, name=f\"w_{i+1}\")\n",
    "            for i in range(3)\n",
    "        ]\n",
    "        epsilon_vars = [\n",
    "            self.model.add_var(var_type=CONTINUOUS, lb=0, name=f\"epsilon_{i+1}\")\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "        x_vars = w_vars + epsilon_vars\n",
    "\n",
    "        # Ajout de la fonction objectif\n",
    "        self.model.objective = minimize(\n",
    "            xsum(c[i] * w_vars[i] for i in range(3))\n",
    "            + self.C * xsum(epsilon_vars)\n",
    "        )\n",
    "\n",
    "        # Ajout des contraintes\n",
    "        for i in range(self.n):\n",
    "            self.model += xsum(A[i][j] * x_vars[j] for j in range(m)) <= b[i]\n",
    "            self.model += x_vars[i + 3] >= 0\n",
    "\n",
    "    def solve(self, verbose=False):\n",
    "        \"\"\"\n",
    "        Résout le modèle MIP.\n",
    "        \"\"\"\n",
    "        super().solve(verbose)\n",
    "        self.epsilons = [v.x for v in self.model.vars[3 : self.n + 3]]\n",
    "\n",
    "    def compute_errors(self):\n",
    "        super().compute_errors()\n",
    "        epsilons = {}\n",
    "        if self.epsilons is not None:\n",
    "            epsilons = {\n",
    "            \"Erreur moyenne slack\" : np.mean(self.epsilons)\n",
    "\t\t\t\t\t}\n",
    "        self.errors.update(epsilons)\n",
    "\n",
    "    @classmethod\n",
    "    def train_multiple_models(cls, points, labels, C_values):\n",
    "        \"\"\"\n",
    "        Entraîne plusieurs modèles SVM avec différentes valeurs de C et retourne leurs résultats.\n",
    "\n",
    "        :param points: Données d'entrée.\n",
    "        :param labels: Étiquettes associées.\n",
    "        :param C_values: Liste des valeurs de C à tester.\n",
    "        :return: Liste des modèles entraînés et liste des erreurs associées.\n",
    "        \"\"\"\n",
    "        trained_models = []\n",
    "        errors = []\n",
    "\n",
    "        for C in C_values:\n",
    "            current_svm = cls(points, labels, C)\n",
    "            current_svm.train_model()\n",
    "            current_svm.compute_errors()\n",
    "            trained_models.append(current_svm)\n",
    "            errors.append(current_svm.errors)\n",
    "\n",
    "        return (\n",
    "            trained_models,\n",
    "            errors,\n",
    "        )  # On retourne les modèles et leurs erreurs\n",
    "\n",
    "    @classmethod\n",
    "    def plot_C_variation(cls, trained_models, C_values, colors, plot_marge=False, plot_support_vectors=False):\n",
    "        \"\"\"\n",
    "        Trace les hyperplans pour les modèles entraînés avec différentes valeurs de C.\n",
    "\n",
    "        :param trained_models: Liste des modèles entraînés (résultats de train_multiple_models).\n",
    "        :param C_values: Liste des valeurs de C utilisées.\n",
    "        \"\"\"\n",
    "\n",
    "        if colors and len(colors) != len(C_values):\n",
    "            print(\"Le nombre de couleurs doit correspondre au nombre de valeurs de C.\")\n",
    "            return\n",
    "        # Générer automatiquement une liste de couleurs si aucune n'est fournie\n",
    "        if not colors:\n",
    "            colors = [cm.viridis(i / len(C_values)) for i in range(len(C_values))]\n",
    "\n",
    "        # Tracer les points\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(\n",
    "            trained_models[0].points[:, 0],\n",
    "            trained_models[0].points[:, 1],\n",
    "            c=trained_models[0].labels,\n",
    "            cmap=\"bwr\",\n",
    "            edgecolors=\"k\",\n",
    "        )\n",
    "\n",
    "        # Tracer les hyperplans et marges\n",
    "        x_min, x_max = (\n",
    "            trained_models[0].points[:, 0].min() - 1,\n",
    "            trained_models[0].points[:, 0].max() + 1,\n",
    "        )\n",
    "        y_min, y_max = (\n",
    "            trained_models[0].points[:, 1].min() - 1,\n",
    "            trained_models[0].points[:, 1].max() + 1,\n",
    "        )\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500)\n",
    "        )\n",
    "\n",
    "        # Liste pour stocker les handles des légendes\n",
    "        legend_handles = []\n",
    "\n",
    "        for i, model in enumerate(trained_models):\n",
    "            if model.w:\n",
    "                w = model.w\n",
    "                C_value = C_values[i]\n",
    "                equation_str = rf\"$\\text{{C =}} {C_value} \\text{{ Equation de l'hyperplan: }} {model.w[0]:.2f}x_1 + {model.w[1]:.2f}x_2 + {model.w[2]:.2f} = 0$\"\n",
    "                display(Math(equation_str))\n",
    "                Z = w[0] * xx + w[1] * yy + w[2]\n",
    "\n",
    "                # Tracer l'hyperplan\n",
    "                plt.contour(\n",
    "                    xx,\n",
    "                    yy,\n",
    "                    Z,\n",
    "                    levels=[0],\n",
    "                    colors=[colors[i]],\n",
    "                    linestyles=\"dashed\",\n",
    "                )\n",
    "                if plot_marge:\n",
    "                    # Tracer les marges (+1 et -1)\n",
    "                    plt.contour(\n",
    "                      xx,\n",
    "                      yy,\n",
    "                      Z,\n",
    "                      levels=sorted([-1, 1]),\n",
    "                      colors=[colors[i]],\n",
    "                      linestyles=\"dotted\",\n",
    "                  )\n",
    "\n",
    "                # Ajouter une légende en traçant un point invisible\n",
    "                legend_handles.append(\n",
    "                    plt.plot(\n",
    "                        [],\n",
    "                        [],\n",
    "                        color=colors[i],\n",
    "                        linestyle=\"dashed\",\n",
    "                        label=f\"C={C_values[i]}\",\n",
    "                    )[0]\n",
    "                )\n",
    "                if plot_support_vectors:\n",
    "                    support_vectors = model.detect_vectors()\n",
    "                    support_vectors = [v.get(\"index\") for v in support_vectors]\n",
    "                    plt.scatter(\n",
    "                        model.points[support_vectors, 0],\n",
    "                        model.points[support_vectors, 1],\n",
    "                        s=100,\n",
    "                        facecolors=\"none\",\n",
    "                        edgecolors=colors[i],\n",
    "                        linewidths=2,\n",
    "                        label=f\"Vecteurs supports C = {C_values[i]}\",\n",
    "                        zorder=3,  # S'assure que les points sont bien affichés au premier plan\n",
    "                    )\n",
    "\n",
    "        # Ajout de la légende et affichage du graphe\n",
    "        plt.title(\n",
    "            \"Effet de C sur l'hyperplan optimal et la taille de la marge\"\n",
    "        )\n",
    "        plt.xlabel(r\"$x_{1}$\")\n",
    "        plt.ylabel(r\"$x_{2}$\")\n",
    "        plt.legend(handles=legend_handles, loc=\"best\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_svm_errors(self, C_values, errors, plot_taux_erreurs=True, plot_mean_slack=True):\n",
    "        \"\"\"\n",
    "        Trace le graphique de l'impact de C sur les erreurs du SVM.\n",
    "\n",
    "        :param C_values: Liste des valeurs de C (hyperparamètre SVM)\n",
    "        :param errors: Liste des dictionnaires contenant les erreurs pour chaque C\n",
    "        \"\"\"\n",
    "\n",
    "        # Vérification des entrées\n",
    "        if len(C_values) != len(errors):\n",
    "            raise ValueError(\n",
    "                \"La longueur de C_values et errors doit être identique.\"\n",
    "            )\n",
    "\n",
    "        # Extraire les valeurs des erreurs\n",
    "        taux_erreurs = [err[\"error rate\"] for err in errors]\n",
    "        mean_slack = [err[\"Erreur moyenne slack\"] for err in errors]\n",
    "\n",
    "        # Tracer le graphique\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        if plot_mean_slack:\n",
    "            plt.plot(\n",
    "                C_values,\n",
    "                mean_slack,\n",
    "                marker=\"o\",\n",
    "                linestyle=\"-\",\n",
    "                label=\"Erreur moyenne slack\",\n",
    "                color=\"red\",\n",
    "            )\n",
    "        if plot_taux_erreurs :\n",
    "            plt.plot(\n",
    "                C_values,\n",
    "                taux_erreurs,\n",
    "                marker=\"s\",\n",
    "                linestyle=\"--\",\n",
    "                label=\"Taux d'erreurs\",\n",
    "                color=\"blue\",\n",
    "            )\n",
    "\n",
    "        # Utilisation d'une échelle logarithmique pour C\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Valeur de C (échelle logarithmique)\")\n",
    "        plt.ylabel(\"Erreur\")\n",
    "        plt.title(\"Impact de C sur les erreurs du SVM\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définissons le modèle de la SVM avec *Hinge Loss* avec la classe `SVMHingeLoss` et affichons sa formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMhingeloss = SVMHingeLoss(points, labels, 0.1)\n",
    "SVMhingeloss.set_model()\n",
    "SVMhingeloss.show_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMhingeloss = SVMHingeLoss(points, labels, 0.1)\n",
    "SVMhingeloss.run_and_plot()\n",
    "SVMhingeloss.compute_errors()\n",
    "print(f\"Taux d'erreurs : {SVMhingeloss.errors.get('error rate')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme en témoigne la figure ci-dessous, avec le dataset généré, et l'yperplan optimal obtenu avec la SVM avec *Hine Loss* : $0.31x + 0.14y + 0.01 = 0$, nous obtenons un taux d'erreur de 25%.\n",
    "\n",
    "Dans une démarche analogue à celle réalisée précédemment, étoffons le propos en affichant les frontières de décision ainsi que les vecteurs supports par l'intermédiaire de la méthode `plot_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMhingeloss.plot_results(plot_marge=True, plot_support_vectors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysons maintenant l'effet de $C$ sur le plan optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse quantitative de l'effet de $C$ sur le plan optimal\n",
    "\n",
    "Nous allons étudier l'impact de la variation de la pondération $C$ utilisée dans les losses sur le plan optimal. Pour ce faire, nous allons itérer sur les valeurs de $C$ suivantes : $[0.1, 0.3, 1, 10]$, chacune représentée par une couleur différente sur le graphique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [0.1, 0.3, 1, 10]\n",
    "colors = [\"orangered\", \"purple\", \"green\", \"cyan\"]\n",
    "trained_models, errors = SVMhingeloss.train_multiple_models(\n",
    "    points, labels, C_values\n",
    ")\n",
    "SVMhingeloss.plot_C_variation(trained_models, C_values, colors, plot_marge=True, plot_support_vectors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'hyperparamètre $C$ dans la SVM avec *Hinge Loss* contrôle le compromis entre maximisation de la marge et minimisation des erreurs de classification.\n",
    "- $C$ faible (par exemple $C = 0.1$) :\n",
    "\t- La SVM maximise la marge entre les classes ;\n",
    "\t- Elle tolère davantage d'erreurs sur le jeu d'entraînement ;\n",
    "\t- Cela entraîne une frontière plus large mais peut conduire à un modèle moins précis sur l'entraînement. **Cependant elle assure une meilleure généralisation sur le jeu de test.**\n",
    "- $C$ élevé (par exemple $C = 10$) :\n",
    "\t- La SVM pénalise fortement les erreurs et essaie de les minimiser ;\n",
    "\t- Cela conduit à une frontière plus stricte ;\n",
    "\t- Cependant, cela peut mener à un sur-ajustement, où le modèle suit trop fidèlement les données d'entraînement et pourrait mal généraliser à de nouvelles données (i.e. **sur-apprentissage**).\n",
    "\n",
    "Comme en témoigne la figure ci-dessus, les marges se rétrécissent lorsque $C$ augmente. En effet, les frontières de décision relatives à la valeur de $C = 10$ (en orange) est plus étroite que celle à $C = 0.1$ qui est plus large (en cyan).\n",
    "\n",
    "Pour étayer ce constat, nous allons calculer le taux d'erreur en fonction de $C$ pour la SVM avec *Hinge Loss*.\n",
    "\n",
    "### Affichage du taux d'erreur en fonction de $C$ pour la SVM avec *Hinge Loss*\n",
    "\n",
    "Etant donné qu'avec le jeu de données dont nous disposons, les éléments relatifs au taux d'erreurs ainsi qu'au nombre d'erreurs sont les mêmes indépendamment de la valeur de $C$, nous allons exploiter l'erreur quadratique moyenne des $\\epsilon_{i}$ (défini préalablement).\n",
    "\n",
    "Pour ce faire, nous allons implémenter une méthode `plot_svm_errors()` qui prendra en paramètre les valeurs de $C$ et les erreurs associées, puis affichera le graphique correspondant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMhingeloss.plot_svm_errors(C_values, errors, plot_taux_erreurs=False, plot_mean_slack=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats présentés dans le graphique ci-dessus sont en adéquation avec le constat réalisé précédemment dans l'analyse quantitative de l'effet de $C$ sur le plan optimal : plus $C$ est élevé, moins les erreurs sont tolérées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation du problème d'optimisation de la SVM avec Ramp Loss et reformulation.\n",
    "\n",
    "Nous cherchons cherche à résoudre le problème d'optimisation suivant :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{w} \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\text{minimiser}}\n",
    "& & \\frac{1}{2} \\|\\mathbf{w}\\| + C \\left( \\sum\\limits_{i=1}^{n} \\epsilon_{i} + 2 \\sum\\limits_{i=1}^{n} z_{i} \\right) \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\epsilon_{i} - M z_{i}, \\quad \\forall i \\in \\{1, \\ldots, n\\} \\\\\n",
    "& & & 0 \\leq \\epsilon_{i} \\leq 2, \\quad \\forall i \\in \\{1, \\ldots, n\\} \\\\\n",
    "& & & z_{i} \\in \\{0, 1\\}, \\quad \\forall i \\in \\{1, \\ldots, n\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "avec $M$ une constante suffisamment grande pour que $z_{i}$ soit égal à $1$ si et seulement si $\\epsilon_{i} > 1$ et $C$ le poids de la minimisation de l'erreur par rapport à la marge.\n",
    "\n",
    "La différence avec la *Hinge Loss* est l'introduction d'une variable binaire $z_{i}$ pour tout $i \\in \\{1, \\ldots, n\\}$ qui vaut :\n",
    "$$\n",
    "z{i} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\text{si } y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\leq -1 \\\\\n",
    "0 & \\text{sinon}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Dans la continuité des travaux réalisés jusqu'à présente, nous pouvons reformuler le problème ci-dessus de la manière suivante (nous poserons de nouveau $b = w_3$):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{(\\mathbf{w_0}, \\mathbf{w_1}, \\mathbf{w_2}) \\in \\mathbb{R}^{3}, \\epsilon_{i} \\in \\mathbb{R}^{+}}{\\text{minimiser}}\n",
    "& & \\frac{1}{2} | \\mathbf{w}_{1} | + \\frac{1}{2} | \\mathbf{w}_{2} | + 0 | \\mathbf{w}_{3} | + C \\left( \\sum\\limits_{i=1}^{n} \\epsilon_{i} + 2 \\sum\\limits_{i=1}^{n} z_{i} \\right) \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & y_i (w_{1} x_{1, i} + w_{2} x_{2, i} + w_{3}) \\geq 1 - \\epsilon_{i} - M z_{i}, \\quad \\forall i \\in \\{1, \\ldots, n\\} \\\\\n",
    "& & & 0 \\leq \\epsilon_{i} \\leq 2, \\quad \\forall i \\in \\{1, \\ldots, n\\} \\\\\n",
    "& & & z_{i} \\in \\{0, 1\\}, \\quad \\forall i \\in \\{1, \\ldots, n\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "En modifiant les contraintes et en reformulant le problème d'optimisation de la SVM avec Hinge Loss, nous obtenons :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{(\\mathbf{w_0}, \\mathbf{w_1}, \\mathbf{w_2}) \\in \\mathbb{R}^{3}, \\epsilon_{i} \\in \\mathbb{R}^{+}}{\\text{minimiser}}\n",
    "& & \\frac{1}{2} | \\mathbf{w}_{1} | + \\frac{1}{2} | \\mathbf{w}_{2} | + 0 | \\mathbf{w}_{3} | + C \\left( \\sum\\limits_{i=1}^{n} \\epsilon_{i} + 2 \\sum\\limits_{i=1}^{n} z_{i} \\right) \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & -y_i (w_{1} x_{1, i} + w_{2} x_{2, i} + w_{3}) - \\epsilon_{i} - M z_{i} \\leq - 1, \\quad \\forall i \\in \\{1, \\ldots, n\\} \\\\\n",
    "& & & 0 \\leq \\epsilon_{i} \\leq 2, \\quad \\forall i \\in \\{1, \\ldots, n\\} \\\\\n",
    "& & & z_{i} \\in \\{0, 1\\}, \\quad \\forall i \\in \\{1, \\ldots, n\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Nous cherchons à exprimer ce problème sous la forme canonique d'un problème d'optimisation linéaire :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{x}}{\\text{minimiser}}\n",
    "& & \\mathbf{c}^{T} \\mathbf{x} \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & A \\mathbf{x} \\leq \\mathbf{b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "avec $\\mathbf{x} = \\begin{bmatrix} \\mathbf{w} \\\\ \\boldsymbol{\\epsilon} \\\\ \\boldsymbol{z} \\end{bmatrix}$, et les matrices définies comme suit :\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} w_{1} \\\\ w_{2} \\\\ w_{3} \\\\ \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\\\ z_{1} \\\\ z_{2} \\\\ \\vdots \\\\ z_{n} \\end{bmatrix}\n",
    "$$\n",
    "avec $\\mathbf{x} \\in \\mathbb{R}^{2n+3}$\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = \\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\\\ C \\\\ C \\\\ \\vdots \\\\ C \\\\ 2C \\\\ 2C \\\\ \\vdots \\\\ 2C \\end{bmatrix}\n",
    "$$\n",
    "avec $\\mathbf{c} \\in \\mathbb{R}^{2n+3}$\n",
    "\n",
    "$$\n",
    "\\mathbf{b} = \\begin{bmatrix} -1 \\\\ -1 \\\\ \\vdots \\\\ -1 \\end{bmatrix}\n",
    "$$\n",
    "avec $\\mathbf{b} \\in \\mathbb{R}^{n}$\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "-y_{1} x_{1,1} & -y_{1} x_{1,2} & -y_{1} & -1 & 0 & \\dots & 0 & -M & 0 & \\dots & 0 \\\\\n",
    "-y_{2} x_{2,1} & -y_{2} x_{2,2} & -y_{2} & 0 & -1 & \\dots & 0 & 0 & -M & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots \\\\\n",
    "-y_{n} x_{n,1} & -y_{n} x_{n,2} & -y_{n} & 0 & 0 & \\dots & -1 & 0 & 0 & \\dots & -M \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "avec $A \\in \\mathbb{R}^{n \\times (2n+3)}$\n",
    "\n",
    "**Ajout de la contrainte $z_{i}$**:\n",
    "D'après la définition de $z_{i}$, donnée ci-dessus, la variable $z_{i}$ doit s'activer uniquement quand un point est fortement mal classé, c'est-à-dire quand il est au moins à distance $-1$ du bon côté de la séparation.\n",
    "Cette contrainte peut se traduire ainsi :\n",
    "$$\n",
    "y_i (w_{1} x_{1, i} + w_{2} x_{2, i} + w_{3}) + M z_{i} \\geq -1\n",
    "$$\n",
    "En effet, si $z_{i} = 0$ (*i.e.* le point est bien classé ou dans la marge), nous obtenons :\n",
    "$$\n",
    "y_i (w_{1} x_{1, i} + w_{2} x_{2, i} + w_{3}) \\geq -1\n",
    "$$\n",
    "Ce qui signifie que le point peut être correctement classé ou dans la marge mais pas trop loin.\n",
    "En revanche, si $z_{i} = 1$ (*i.e.* le point est mal classé ou dans la marge), l'inéquation devient :\n",
    "$$\n",
    "y_i (w_{1} x_{1, i} + w_{2} x_{2, i} + w_{3}) + M \\geq -1\n",
    "$$\n",
    "Comme $M$ est pris suffisament grand, comme par exemple 1000, cette contrainte est toujours vérifiée quelle que soit la valeur de $y_{i} (w_{1} x_{1, i} + w_{2} x_{2, i} + w_{3})$.\n",
    "Tous ces éléments indiquent que $M$ relâche la contrainte quand $z_{i} = 1$ pour que l'optimisation puisse \"accepter\" les erreurs sévères. Et $z_{i}$ est forcé à $1$ si et seulement si $y_{i} (w_{1} x_{1, i} + w_{2} x_{2, i} + w_{3}) \\leq -1$.\n",
    "\n",
    "Dans le cadre de l'implémentation, cette contrainte se caractérise ainsi :\n",
    "```python\n",
    "model += (\n",
    "        y[i] * (w_vars[0] * x[i][0] + w_vars[1] * x[i][1] + w_vars[2])\n",
    "        + M * z_vars[i]\n",
    "        >= -1\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMRampLoss(SVMBase):\n",
    "    \"\"\"\n",
    "    Implémentation du SVM avec Ramp Loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, points, labels, C=1, M=1000):\n",
    "        \"\"\"\n",
    "        Initialise le SVM avec Hinge Loss.\n",
    "        \"\"\"\n",
    "        super().__init__(points, labels)\n",
    "        self.C = C\n",
    "        self.M = M\n",
    "\n",
    "    def set_model(self):\n",
    "        \"\"\"\n",
    "        Crée le modèle MIP pour le SVM avec Hinge Loss.\n",
    "        \"\"\"\n",
    "        x = self.points\n",
    "        y = self.labels\n",
    "        m = 2 * self.n + 3  # Nombre total de variables\n",
    "        c = [0.5, 0.5, 0] + [self.C for _ in range(self.n)] + [2 * self.C for _ in range(self.n)] # Vecteur des coûts\n",
    "        b = [-1 for _ in range(self.n)] # Vecteur des contraintes\n",
    "        A = [] # Matrice des contraintes\n",
    "        for i in range(self.n):\n",
    "            l = [-y[i] * x[i][0], -y[i] * x[i][1], -y[i]]  # w1, w2, w3\n",
    "            l += [-1 if j == i else 0 for j in range(self.n)]  # epsilon_i\n",
    "            l += [-self.M if j == i else 0 for j in range(self.n)]  # z_i\n",
    "            A.append(l)\n",
    "        # création du modèle\n",
    "        self.model = Model(\"Ramp Loss\")\n",
    "        # variables continues : w1, w2, w3, ε_i\n",
    "        w_vars = [self.model.add_var(var_type=CONTINUOUS, name=f\"w_{i+1}\") for i in range(3)]\n",
    "        epsilon_vars = [\n",
    "            self.model.add_var(var_type=CONTINUOUS, name=f\"epsilon_{i+1}\")\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "        # variables binaires : z_i\n",
    "        z_vars = [self.model.add_var(var_type=BINARY, name=f\"z_{i+1}\") for i in range(self.n)]\n",
    "        # variables x qui concatène w, ε et z\n",
    "        x_vars = w_vars + epsilon_vars + z_vars\n",
    "        # ajout de la fonction objectif\n",
    "        # séparation des w_vars, epsilons_vars et z_vars pour une meilleure lisibilité\n",
    "        self.model.objective = minimize(\n",
    "            xsum(c[i] * w_vars[i] for i in range(3))\n",
    "            + self.C * xsum(epsilon_vars)\n",
    "            + 2 * self.C * xsum(z_vars)\n",
    "        )\n",
    "        # Ajout des contraintes\n",
    "        for i in range(self.n):\n",
    "            self.model += xsum(A[i][j] * x_vars[j] for j in range(m)) <= b[i]\n",
    "            # 0 <= $\\epsilon_i$ <= 2\n",
    "            # (le décalage de 3 provient de la présence des variables w_1, w_2 et w_3)\n",
    "            self.model += x_vars[i + 3] >= 0\n",
    "            self.model += x_vars[i + 3] <= 2\n",
    "            # contraintes sur z_i\n",
    "            self.model += (\n",
    "                y[i] * (w_vars[0] * x[i][0] + w_vars[1] * x[i][1] + w_vars[2])\n",
    "                + self.M * z_vars[i]\n",
    "                >= -1\n",
    "            )\n",
    "            # Ajout explicite que z_i est binaire (déjà garanti par BINARY mais assure\n",
    "            # une meilleure lisibilité)\n",
    "            self.model += z_vars[i] >= 0\n",
    "            self.model += z_vars[i] <= 1\n",
    "\n",
    "    def solve(self, verbose=False):\n",
    "        \"\"\"\n",
    "        Résout le modèle MIP.\n",
    "        \"\"\"\n",
    "        super().solve(verbose)\n",
    "        self.epsilons = [v.x for v in self.model.vars[3 : self.n + 3]]\n",
    "        self.z = [v.x for v in self.model.vars[self.n + 3:]]\n",
    "\n",
    "    def compute_errors(self):\n",
    "        super().compute_errors()\n",
    "        epsilons = {}\n",
    "        if self.epsilons is not None:\n",
    "            epsilons = {\"Erreur moyenne slack\": np.mean(self.epsilons)}\n",
    "        self.errors.update(epsilons)\n",
    "\n",
    "    @classmethod\n",
    "    def train_multiple_models(cls, points, labels, C_values):\n",
    "        \"\"\"\n",
    "        Entraîne plusieurs modèles SVM avec différentes valeurs de C et retourne leurs résultats.\n",
    "\n",
    "        :param points: Données d'entrée.\n",
    "        :param labels: Étiquettes associées.\n",
    "        :param C_values: Liste des valeurs de C à tester.\n",
    "        :return: Liste des modèles entraînés et liste des erreurs associées.\n",
    "        \"\"\"\n",
    "        trained_models = []\n",
    "        errors = []\n",
    "\n",
    "        for C in C_values:\n",
    "            current_svm = cls(points, labels, C)\n",
    "            current_svm.train_model()\n",
    "            current_svm.compute_errors()\n",
    "            trained_models.append(current_svm)\n",
    "            errors.append(current_svm.errors)\n",
    "\n",
    "        return (\n",
    "            trained_models,\n",
    "            errors,\n",
    "        )  # On retourne les modèles et leurs erreurs\n",
    "\n",
    "    @classmethod\n",
    "    def plot_C_variation(cls, trained_models, C_values, colors, plot_marge=False, plot_support_vectors=False):\n",
    "        \"\"\"\n",
    "        Trace les hyperplans pour les modèles entraînés avec différentes valeurs de C.\n",
    "\n",
    "        :param trained_models: Liste des modèles entraînés (résultats de train_multiple_models).\n",
    "        :param C_values: Liste des valeurs de C utilisées.\n",
    "        \"\"\"\n",
    "\n",
    "        if colors and len(colors) != len(C_values):\n",
    "            print(\n",
    "                \"Le nombre de couleurs doit correspondre au nombre de valeurs de C.\"\n",
    "            )\n",
    "            return\n",
    "        # Générer automatiquement une liste de couleurs si aucune n'est fournie\n",
    "        if not colors:\n",
    "            colors = [\n",
    "                cm.viridis(i / len(C_values)) for i in range(len(C_values))\n",
    "            ]\n",
    "\n",
    "        # Tracer les points\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(\n",
    "            trained_models[0].points[:, 0],\n",
    "            trained_models[0].points[:, 1],\n",
    "            c=trained_models[0].labels,\n",
    "            cmap=\"bwr\",\n",
    "            edgecolors=\"k\",\n",
    "        )\n",
    "\n",
    "        # Tracer les hyperplans et marges\n",
    "        x_min, x_max = (\n",
    "            trained_models[0].points[:, 0].min() - 1,\n",
    "            trained_models[0].points[:, 0].max() + 1,\n",
    "        )\n",
    "        y_min, y_max = (\n",
    "            trained_models[0].points[:, 1].min() - 1,\n",
    "            trained_models[0].points[:, 1].max() + 1,\n",
    "        )\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500)\n",
    "        )\n",
    "\n",
    "        # Liste pour stocker les handles des légendes\n",
    "        legend_handles = []\n",
    "\n",
    "        for i, model in enumerate(trained_models):\n",
    "            if model.w:\n",
    "                w = model.w\n",
    "                C_value = C_values[i]\n",
    "                equation_str = rf\"$\\text{{C =}} {C_value} \\text{{ Equation de l'hyperplan: }} {model.w[0]:.2f}x_1 + {model.w[1]:.2f}x_2 + {model.w[2]:.2f} = 0$\"\n",
    "                display(Math(equation_str))\n",
    "                Z = w[0] * xx + w[1] * yy + w[2]\n",
    "\n",
    "                # Tracer l'hyperplan\n",
    "                plt.contour(\n",
    "                    xx,\n",
    "                    yy,\n",
    "                    Z,\n",
    "                    levels=[0],\n",
    "                    colors=[colors[i]],\n",
    "                    linestyles=\"dashed\",\n",
    "                )\n",
    "\n",
    "                if plot_marge:\n",
    "                    # Tracer les marges (+1 et -1)\n",
    "                    plt.contour(\n",
    "                        xx,\n",
    "                        yy,\n",
    "                        Z,\n",
    "                        levels=sorted([-1, 1]),\n",
    "                        colors=[colors[i]],\n",
    "                        linestyles=\"dotted\",\n",
    "                    )\n",
    "\n",
    "                # Ajouter une légende en traçant un point invisible\n",
    "                legend_handles.append(\n",
    "                    plt.plot(\n",
    "                        [],\n",
    "                        [],\n",
    "                        color=colors[i],\n",
    "                        linestyle=\"dashed\",\n",
    "                        label=f\"C={C_values[i]}\",\n",
    "                    )[0]\n",
    "                )\n",
    "                if plot_support_vectors:\n",
    "                    support_vectors = model.detect_vectors()\n",
    "                    support_vectors = [v.get(\"index\") for v in support_vectors]\n",
    "                    plt.scatter(\n",
    "                        model.points[support_vectors, 0],\n",
    "                        model.points[support_vectors, 1],\n",
    "                        s=100,\n",
    "                        facecolors=\"none\",\n",
    "                        edgecolors=colors[i],\n",
    "                        linewidths=2,\n",
    "                        label=f\"Vecteurs supports C = {C_values[i]}\",\n",
    "                        zorder=3,  # S'assure que les points sont bien affichés au premier plan\n",
    "                    )\n",
    "\n",
    "                # Ajouter une légende en traçant un point invisible\n",
    "                legend_handles.append(\n",
    "                    plt.plot(\n",
    "                        [],\n",
    "                        [],\n",
    "                        color=colors[i],\n",
    "                        linestyle=\"dashed\",\n",
    "                        label=f\"C={C_values[i]}\",\n",
    "                    )[0]\n",
    "                )\n",
    "\n",
    "        # Ajout de la légende et affichage du graphe\n",
    "        plt.title(\n",
    "            \"Effet de C sur l'hyperplan optimal et la taille de la marge\"\n",
    "        )\n",
    "        plt.xlabel(r\"$x_{1}$\")\n",
    "        plt.ylabel(r\"$x_{2}$\")\n",
    "        plt.legend(handles=legend_handles, loc=\"best\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_svm_errors(\n",
    "      C_values, errors, plot_taux_erreurs=True, plot_mean_slack=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trace le graphique de l'impact de C sur les erreurs du SVM.\n",
    "\n",
    "        :param C_values: Liste des valeurs de C (hyperparamètre SVM)\n",
    "        :param errors: Liste des dictionnaires contenant les erreurs pour chaque C\n",
    "        \"\"\"\n",
    "\n",
    "        # Vérification des entrées\n",
    "        if len(C_values) != len(errors):\n",
    "            raise ValueError(\n",
    "                \"La longueur de C_values et errors doit être identique.\"\n",
    "            )\n",
    "\n",
    "        # Extraire les valeurs des erreurs\n",
    "        taux_erreurs = [err[\"error rate\"] for err in errors]\n",
    "        mean_slack = [err[\"Erreur moyenne slack\"] for err in errors]\n",
    "\n",
    "        # Tracer le graphique\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        if plot_mean_slack:\n",
    "            plt.plot(\n",
    "                C_values,\n",
    "                mean_slack,\n",
    "                marker=\"o\",\n",
    "                linestyle=\"-\",\n",
    "                label=\"Erreur moyenne slack\",\n",
    "                color=\"red\",\n",
    "            )\n",
    "        if plot_taux_erreurs:\n",
    "            plt.plot(\n",
    "                C_values,\n",
    "                taux_erreurs,\n",
    "                marker=\"s\",\n",
    "                linestyle=\"--\",\n",
    "                label=\"Taux d'erreurs\",\n",
    "                color=\"blue\",\n",
    "            )\n",
    "\n",
    "        # Utilisation d'une échelle logarithmique pour C\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Valeur de C (échelle logarithmique)\")\n",
    "        plt.ylabel(\"Erreur\")\n",
    "        plt.title(\"Impact de C sur les erreurs du SVM\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMramploss = SVMRampLoss(points, labels, 0.1, 1000)\n",
    "SVMramploss.set_model()\n",
    "SVMramploss.show_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMramploss.run_and_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMramploss.plot_results(plot_marge=True, plot_support_vectors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse quantitative de l'effet de $C$ sur le plan optimal dans le cadre de la SVM avec *Ramp Loss*\n",
    "\n",
    "Nous allons réaliser la démarche analogue à celle réalisée dans le cadre de l'analyse quantitative de l'effet de $C$ sur le plan optimal dans le cadre de la SVM avec *Hinge Loss*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [0.1, 0.3, 1, 10]\n",
    "colors = [\"orangered\", \"purple\", \"green\", \"cyan\"]\n",
    "trained_models, errors = SVMRampLoss.train_multiple_models(\n",
    "    points, labels, C_values\n",
    ")\n",
    "SVMRampLoss.plot_C_variation(trained_models, C_values, colors, plot_marge=True, plot_support_vectors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le constat est analogue à celui réalisé dans le cadre de la SVM avec *Hine Loss* : plus la valeur de $C$ est élévée plus la frontière de décision est stricte et moins les erreurs sont tolérées. En effet, la marges de décision associée à la valeur de $C = 0.1$ en cyan est plus large que celle associée à la valeur de $C = 10$ en orange.\n",
    "Fait intéressant, étant donné que nous disposons de peu de points les frontières de décision tendent à devenir horizontales à mesure que $C$ augmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMRampLoss.plot_svm_errors(\n",
    "    C_values=C_values, errors=errors, plot_taux_erreurs=False, plot_mean_slack=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le constat est également similaire à celui réalisé dans le cadre de la SVM avec *Hinge Loss* : plus $C$ est élevé, moins les erreurs sont tolérées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation du problème d'optimisation de la SVM avec *Hard Margin Loss* et reformulation.\n",
    "\n",
    "Nous cherchons cherche à résoudre le problème d'optimisation suivant :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{w} \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\text{minimiser}}\n",
    "& & \\frac{1}{2} \\|\\mathbf{w}\\| + C \\left(2 \\sum\\limits_{i=1}^{n} z_{i} \\right) \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - M z_{i}, \\quad \\forall i \\in \\{1, \\ldots, n\\} \\\\\n",
    "& & & z_{i} \\in \\{0, 1\\}, \\quad \\forall i \\in \\{1, \\ldots, n\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "En s'appuyant sur la travail réalisé précédemment il est aisé reformuler le problème d'optimisation de la SVM avec *Hard Margin Loss* :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{(\\mathbf{w_0}, \\mathbf{w_1}, \\mathbf{w_2}) \\in \\mathbb{R}^{3}, \\epsilon_{i} \\in \\mathbb{R}^{+}}{\\text{minimiser}}\n",
    "& & \\frac{1}{2} | \\mathbf{w}_{1} | + \\frac{1}{2} | \\mathbf{w}_{2} | + 0 | \\mathbf{w}_{3} | + C \\left(2 \\sum\\limits_{i=1}^{n} z_{i} \\right) \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & -y_i (w_{1} x_{1, i} + w_{2} x_{2, i} + w_{3}) - M z_{i} \\leq - 1, \\quad \\forall i \\in \\{1, \\ldots, n\\} \\\\\n",
    "& & & z_{i} \\in \\{0, 1\\}, \\quad \\forall i \\in \\{1, \\ldots, n\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Dans la continuité de nos raisonnements précédents, nous cherchons à exprimer ce problème sous la forme canonique d'un problème d'optimisation linéaire :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\mathbf{x}}{\\text{minimiser}}\n",
    "& & \\mathbf{c}^{T} \\mathbf{x} \\\\\n",
    "& \\text{sous les contraintes}\n",
    "& & A \\mathbf{x} \\leq \\mathbf{b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "avec $\\mathbf{x} = \\begin{bmatrix} \\mathbf{w} \\\\ \\boldsymbol{z} \\end{bmatrix}$, et les matrices définies comme suit :\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} w_{1} \\\\ w_{2} \\\\ w_{3} \\\\ z_{1} \\\\ z_{2} \\\\ \\vdots \\\\ z_{n} \\end{bmatrix}\n",
    "$$\n",
    "avec $\\mathbf{x} \\in \\mathbb{R}^{n+3}$\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = \\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\\\ 2C \\\\ 2C \\\\ \\vdots \\\\ 2C \\end{bmatrix}\n",
    "$$\n",
    "avec $\\mathbf{c} \\in \\mathbb{R}^{n+3}$\n",
    "\n",
    "$$\n",
    "\\mathbf{b} = \\begin{bmatrix} -1 \\\\ -1 \\\\ \\vdots \\\\ -1 \\end{bmatrix}\n",
    "$$\n",
    "avec $\\mathbf{b} \\in \\mathbb{R}^{n}$\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "-y_{1} x_{1,1} & -y_{1} x_{1,2} & -y_{1} & -M & 0 & \\dots & 0 \\\\\n",
    "-y_{2} x_{2,1} & -y_{2} x_{2,2} & -y_{2} & 0 & -M & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-y_{n} x_{n,1} & -y_{n} x_{n,2} & -y_{n} & 0 & 0 & \\dots & -M \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "avec $A \\in \\mathbb{R}^{n \\times (n+3)}$\n",
    "\n",
    "### Lien entre la formulation du problème de SVM à marge rigide classique et celle avec *Hard Margin Loss*\n",
    "\n",
    "Pour rappel, les formulations respectives des problèmes d'optimisation de la SVM à marge rigide et de la SVM avec *Hard Margin Loss* sont les suivantes :\n",
    "1. SVM à marge rigide classique :\n",
    "  - L'objectif est de minimiser $\\frac{1}{2} \\Vert w \\Vert_{1}$ sous la contrainte stricte que tous les points soient classés correctement et à l'extérieur de la marge ;\n",
    "\t- Il n'y a aucune tolérance aux erreurs de classification ;\n",
    "2. SVM à marge rigide avec *Hard Margin Loss* :\n",
    "  - Dans cette formulation sont introduits les variables binaires $z_{i}$ qui permettent de relâcher la contrainte de marge pour certains points ;\n",
    "\t- Si un point $x_{i}$ ne respecte pas la contrainte de marge, alors $z_{i} = 1$ et une pénalité $2 C z_{i}$ est ajoutée dans la fonction objectif ;\n",
    "\t- Cette formulation est similaire à la SVM à marge rigide classique mais permet de tolérer des erreurs de classification pour certains points.\n",
    "\n",
    "Ainsi, dans le cas de points non linéairement séparables, la formulation de la SVM à marge rigide classique ne fournit pas de solutions.\n",
    "\n",
    "Pour cela vérifions le par l'intermédiaire d'une opération `try` / `except` qui permettra de *catch* l'exception relative à l'absence de solution pour la SVM à marge rigide classique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    SVMhardmarginclassical = SVMHardMarginClassical(points, labels)\n",
    "    SVMhardmarginclassical.run_and_plot()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(f\"{SVMhardmarginclassical.w=}\")\n",
    "    print(f\"{SVMhardmarginclassical.model.status=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons bien que pour un *dataset* non linéairement séparable, la SVM à marge rigide classique ne parvient pas à trouver de solution. En effet, à l’issue de l’entraînement, le modèle affiche un état `INFEASIBLE` (cf. `model.status`), indiquant qu’aucune solution réalisable n’existe. De plus, le vecteur `w` ne contient que des valeurs `None`, confirmant l'absence d'optimisation. L’exception levée `(e)` résulte directement de cette situation, car `w` étant `None`, toute tentative d’accès à ses éléments entraîne une erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMHardMarginLoss(SVMBase):\n",
    "    \"\"\"\n",
    "    Implémentation du SVM avec Hard Margin Loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, points, labels, C=1, M=1000):\n",
    "        \"\"\"\n",
    "        Initialise le SVM avec Hinge Loss.\n",
    "        \"\"\"\n",
    "        super().__init__(points, labels)\n",
    "        self.C = C\n",
    "        self.M = M\n",
    "\n",
    "    def set_model(self):\n",
    "        \"\"\"\n",
    "        Crée le modèle MIP pour le SVM avec Hard Margin Loss.\n",
    "        \"\"\"\n",
    "        x = self.points\n",
    "        y = self.labels\n",
    "        m = self.n + 3  # Nombre total de variables\n",
    "        c = [0.5, 0.5, 0] + [2 * self.C for _ in range(self.n)]\t# Vecteur des coûts\n",
    "        b = [-1 for _ in range(self.n)]\t# Vecteur des contraintes\n",
    "        A = []\t# Matrice des contraintes\n",
    "        for i in range(self.n):\n",
    "            l = [-y[i] * x[i][0], -y[i] * x[i][1], -y[i]]  # w1, w2, w3\n",
    "            l += [-self.M if j == i else 0 for j in range(self.n)]  # z_i\n",
    "            A.append(l)\n",
    "        # création du modèle\n",
    "        self.model = Model(\"Hard Margin Loss\")\n",
    "        # variables continues : w1, w2, w3\n",
    "        w_vars = [self.model.add_var(var_type=CONTINUOUS, name=f\"w_{i+1}\") for i in range(3)]\n",
    "        # variables binaires : z_i\n",
    "        z_vars = [self.model.add_var(var_type=BINARY, name=f\"z_{i+1}\") for i in range(self.n)]\n",
    "        # variables x qui concatène w et z\n",
    "        x_vars = w_vars + z_vars\n",
    "        # ajout de la fonction objectif\n",
    "        # séparation des w_vars et z_vars pour une meilleure lisibilité\n",
    "        self.model.objective = minimize(\n",
    "            xsum(c[i] * w_vars[i] for i in range(3)) + 2 * self.C * xsum(z_vars)\n",
    "        )\n",
    "        # Ajout des contraintes\n",
    "        for i in range(self.n):\n",
    "            self.model += xsum(A[i][j] * x_vars[j] for j in range(m)) <= b[i]\n",
    "            # contraintes sur z_i\n",
    "            self.model += (\n",
    "                - y[i] * (w_vars[0] * x[i][0] + w_vars[1] * x[i][1] + w_vars[2])\n",
    "                - self.M * z_vars[i]\n",
    "                <= -1\n",
    "            )\n",
    "            # Ajout explicite que z_i est binaire (déjà garanti par BINARY mais assure\n",
    "            # une meilleure lisibilité)\n",
    "            self.model += z_vars[i] >= 0\n",
    "            self.model += z_vars[i] <= 1\n",
    "\n",
    "    def solve(self, verbose=False):\n",
    "        \"\"\"\n",
    "        Résout le modèle MIP.\n",
    "        \"\"\"\n",
    "        super().solve(verbose)\n",
    "        self.z = [v.x for v in self.model.vars[3 : self.n + 3]]\n",
    "\n",
    "    def compute_errors(self):\n",
    "        super().compute_errors()\n",
    "        z_values = {}\n",
    "        if self.z is not None:\n",
    "            z_values = {\"Erreur moyenne relachement\": np.mean(self.z)}\n",
    "        self.errors.update(z_values)\n",
    "\n",
    "    @classmethod\n",
    "    def train_multiple_models(cls, points, labels, C_values):\n",
    "        \"\"\"\n",
    "        Entraîne plusieurs modèles SVM avec différentes valeurs de C et retourne leurs résultats.\n",
    "\n",
    "        :param points: Données d'entrée.\n",
    "        :param labels: Étiquettes associées.\n",
    "        :param C_values: Liste des valeurs de C à tester.\n",
    "        :return: Liste des modèles entraînés et liste des erreurs associées.\n",
    "        \"\"\"\n",
    "        trained_models = []\n",
    "        errors = []\n",
    "\n",
    "        for C in C_values:\n",
    "            current_svm = cls(points, labels, C)\n",
    "            current_svm.train_model()\n",
    "            current_svm.compute_errors()\n",
    "            trained_models.append(current_svm)\n",
    "            errors.append(current_svm.errors)\n",
    "\n",
    "        return (\n",
    "            trained_models,\n",
    "            errors,\n",
    "        )  # On retourne les modèles et leurs erreurs\n",
    "\n",
    "    @classmethod\n",
    "    def plot_C_variation(cls, trained_models, C_values, colors, plot_marge=False, plot_support_vectors=False):\n",
    "        \"\"\"\n",
    "        Trace les hyperplans pour les modèles entraînés avec différentes valeurs de C.\n",
    "\n",
    "        :param trained_models: Liste des modèles entraînés (résultats de train_multiple_models).\n",
    "        :param C_values: Liste des valeurs de C utilisées.\n",
    "        \"\"\"\n",
    "\n",
    "        if colors and len(colors) != len(C_values):\n",
    "            print(\n",
    "                \"Le nombre de couleurs doit correspondre au nombre de valeurs de C.\"\n",
    "            )\n",
    "            return\n",
    "        # Générer automatiquement une liste de couleurs si aucune n'est fournie\n",
    "        if not colors:\n",
    "            colors = [\n",
    "                cm.viridis(i / len(C_values)) for i in range(len(C_values))\n",
    "            ]\n",
    "\n",
    "        # Tracer les points\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(\n",
    "            trained_models[0].points[:, 0],\n",
    "            trained_models[0].points[:, 1],\n",
    "            c=trained_models[0].labels,\n",
    "            cmap=\"bwr\",\n",
    "            edgecolors=\"k\",\n",
    "        )\n",
    "\n",
    "        # Tracer les hyperplans et marges\n",
    "        x_min, x_max = (\n",
    "            trained_models[0].points[:, 0].min() - 1,\n",
    "            trained_models[0].points[:, 0].max() + 1,\n",
    "        )\n",
    "        y_min, y_max = (\n",
    "            trained_models[0].points[:, 1].min() - 1,\n",
    "            trained_models[0].points[:, 1].max() + 1,\n",
    "        )\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500)\n",
    "        )\n",
    "\n",
    "        # Liste pour stocker les handles des légendes\n",
    "        legend_handles = []\n",
    "\n",
    "        for i, model in enumerate(trained_models):\n",
    "            if model.w:\n",
    "                w = model.w\n",
    "                C_value = C_values[i]\n",
    "                equation_str = rf\"$\\text{{C =}} {C_value} \\text{{ Equation de l'hyperplan: }} {model.w[0]:.2f}x_1 + {model.w[1]:.2f}x_2 + {model.w[2]:.2f} = 0$\"\n",
    "                display(Math(equation_str))\n",
    "                Z = w[0] * xx + w[1] * yy + w[2]\n",
    "\n",
    "                # Tracer l'hyperplan\n",
    "                plt.contour(\n",
    "                    xx,\n",
    "                    yy,\n",
    "                    Z,\n",
    "                    levels=[0],\n",
    "                    colors=[colors[i]],\n",
    "                    linestyles=\"dashed\",\n",
    "                )\n",
    "                if plot_marge:\n",
    "                    # Tracer les marges (+1 et -1)\n",
    "                    plt.contour(\n",
    "                        xx,\n",
    "                        yy,\n",
    "                        Z,\n",
    "                        levels=sorted([-1, 1]),\n",
    "                        colors=[colors[i]],\n",
    "                        linestyles=\"dotted\",\n",
    "                    )\n",
    "\n",
    "                # Ajouter une légende en traçant un point invisible\n",
    "                legend_handles.append(\n",
    "                    plt.plot(\n",
    "                        [],\n",
    "                        [],\n",
    "                        color=colors[i],\n",
    "                        linestyle=\"dashed\",\n",
    "                        label=f\"C={C_values[i]}\",\n",
    "                    )[0]\n",
    "                )\n",
    "                if plot_support_vectors:\n",
    "                    support_vectors = model.detect_vectors()\n",
    "                    support_vectors = [v.get(\"index\") for v in support_vectors]\n",
    "                    plt.scatter(\n",
    "                        model.points[support_vectors, 0],\n",
    "                        model.points[support_vectors, 1],\n",
    "                        s=100,\n",
    "                        facecolors=\"none\",\n",
    "                        edgecolors=colors[i],\n",
    "                        linewidths=2,\n",
    "                        label=f\"Vecteurs supports C = {C_values[i]}\",\n",
    "                        zorder=3,  # S'assure que les points sont bien affichés au premier plan\n",
    "                    )\n",
    "\n",
    "                # Ajouter une légende en traçant un point invisible\n",
    "                legend_handles.append(\n",
    "                    plt.plot(\n",
    "                        [],\n",
    "                        [],\n",
    "                        color=colors[i],\n",
    "                        linestyle=\"dashed\",\n",
    "                        label=f\"C={C_values[i]}\",\n",
    "                    )[0]\n",
    "                )\n",
    "\n",
    "        # Ajout de la légende et affichage du graphe\n",
    "        plt.title(\n",
    "            \"Effet de C sur l'hyperplan optimal et la taille de la marge\"\n",
    "        )\n",
    "        plt.xlabel(r\"$x_{1}$\")\n",
    "        plt.ylabel(r\"$x_{2}$\")\n",
    "        plt.legend(handles=legend_handles, loc=\"best\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_svm_errors(\n",
    "        C_values, errors, plot_taux_erreurs=True, plot_mean_relachement=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trace le graphique de l'impact de C sur les erreurs du SVM.\n",
    "\n",
    "        :param C_values: Liste des valeurs de C (hyperparamètre SVM)\n",
    "        :param errors: Liste des dictionnaires contenant les erreurs pour chaque C\n",
    "        \"\"\"\n",
    "\n",
    "        # Vérification des entrées\n",
    "        if len(C_values) != len(errors):\n",
    "            raise ValueError(\n",
    "                \"La longueur de C_values et errors doit être identique.\"\n",
    "            )\n",
    "\n",
    "        # Extraire les valeurs des erreurs\n",
    "        taux_erreurs = [err[\"error rate\"] for err in errors]\n",
    "        mean_relachement = [err[\"Erreur moyenne relachement\"] for err in errors]\n",
    "\n",
    "        # Tracer le graphique\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        if plot_mean_relachement:\n",
    "            plt.plot(\n",
    "                C_values,\n",
    "                mean_relachement,\n",
    "                marker=\"o\",\n",
    "                linestyle=\"-\",\n",
    "                label=\"Erreur moyenne relachement\",\n",
    "                color=\"red\",\n",
    "            )\n",
    "        if plot_taux_erreurs:\n",
    "            plt.plot(\n",
    "                C_values,\n",
    "                taux_erreurs,\n",
    "                marker=\"s\",\n",
    "                linestyle=\"--\",\n",
    "                label=\"Taux d'erreurs\",\n",
    "                color=\"blue\",\n",
    "            )\n",
    "\n",
    "        # Utilisation d'une échelle logarithmique pour C\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Valeur de C (échelle logarithmique)\")\n",
    "        plt.ylabel(\"Erreur\")\n",
    "        plt.title(\"Impact de C sur les erreurs du SVM\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMhardmarginloss = SVMHardMarginLoss(points, labels, 0.1, 1000)\n",
    "SVMhardmarginloss.set_model()\n",
    "SVMhardmarginloss.show_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMhardmarginloss.run_and_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'hyperplan obtenu avec la SVM à marge rigide avec Hard Margin Loss est donné par l'équation $0.52x + 0.64y + 0.00 = 0$.\n",
    "Fait intéressant, cet hyperplan est identique à celui obtenu avec la SVM à marge rigide classique. Cependant, étant donné que les points ne sont pas linéairement séparables, la SVM à marge rigide classique n'aurait pas été en mesure de fournir une solution (comme expliqué précédemment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMhardmarginloss.plot_results(plot_marge=True, plot_support_vectors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse quantitative de l'effet de $C$ sur le plan optimal dans le cadre de la SVM avec *Hard Margin Loss*\n",
    "\n",
    "Nous allons réaliser la démarche analogue à celle réalisée dans le cadre de l'analyse quantitative de l'effet de $C$ sur le plan optimal dans le cadre de la SVM avec *Hard Margin Loss*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [0.1, 0.3, 1, 10]\n",
    "colors = [\"orangered\", \"purple\", \"green\", \"cyan\"]\n",
    "trained_models, errors = SVMhardmarginloss.train_multiple_models(\n",
    "    points, labels, C_values\n",
    ")\n",
    "SVMhardmarginloss.plot_C_variation(trained_models, C_values, colors, plot_marge=True, plot_support_vectors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'hyperplan obtenu est le même quelque soit les valeurs prises par $C$ ($0.1$ , $0.3$, $1$ et $10$).\n",
    "\n",
    "Nous allons donc affiner ces valeurs pour vérifier qu'à mesure que la valeur de $C$ augmente, la frontière de décision diminue (comme c'est le cas pour les autres modèles préalablement exposés).\n",
    "\n",
    "Ainsi, nous allons exploiter les valeurs de $C$ suivantes : $00.4$ , $0.1$, $1$ et $10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [0.04, 0.1, 1, 10]\n",
    "colors = [\"orangered\", \"purple\", \"green\", \"cyan\"]\n",
    "trained_models, errors = SVMhardmarginloss.train_multiple_models(\n",
    "    points, labels, C_values\n",
    ")\n",
    "SVMhardmarginloss.plot_C_variation(trained_models, C_values, colors, plot_marge=True, plot_support_vectors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons qu’avec $C = 0.04$, la frontière de décision est plus large, ce qui permet une plus grande tolérance aux erreurs. En revanche, à partir de $C = 0.1$ et jusqu'à $C = 10$, l’hyperplan reste identique mais devient moins étendu, indiquant une réduction de la tolérance aux erreurs par rapport à $C = 0.04$.\n",
    "C'est la raison pour laquelle les frontières de décision ainsi que la marge relatives aux valeur de $C = 0.1$ et $C = 10$ sont dissimulées par celles relatives à $C = 10$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMhardmarginloss.plot_svm_errors(\n",
    "    C_values=C_values,\n",
    "    errors=errors,\n",
    "    plot_taux_erreurs=False,\n",
    "    plot_mean_relachement=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour $C = 0.04$, l'erreur est plus élevée. En revanche, à partir de $C=0.1$ et jusqu'à $C=10$, l'erreur reste constante mais à un niveau plus faible qu'avec $C=0.04$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison des modèles de SVM\n",
    "\n",
    "Nous allons comparer les performances des différents modèles de SVM implémentés.\n",
    "\n",
    "Pour cela, nous allons effectuer un raisonnement analogue à ceux réalisés précédemment pour chacun des modèles. Mais pour cela nous utiliserons un jeu de données beaucoup plus important et, dans le cadre de la compraison des erreur, nous utiliserons exclusivement le taux d'erreurs des points mal classés, définis dans la classe abstraite `SVMBase`, qui, pour rappel, s'exprime ainsi : $\\frac{\\sum\\limits_{i=1}^{n} \\mathbb{1}_{y_i \\times (w_{1} x_{i, 1} + w_{2} x_{i, 2} + w_{3}) \\; < \\; 0}}{n} \\times 100$.\n",
    "\n",
    "Cela permettra de disposer d'un élément commun à chaque modèle pour les comparer.\n",
    "\n",
    "Nous allons générer un *dataset* de 500 points avec du bruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Générer des données gaussiennes\n",
    "n_samples = 200  # Nombre de points\n",
    "centers = [(-1, -1), (1, 1)]  # Centres des clusters\n",
    "X, y = make_blobs(\n",
    "    n_samples=n_samples, centers=centers, cluster_std=1.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convertir les labels pour correspondre aux SVM (-1 et 1)\n",
    "y = np.where(y == 0, -1, 1)\n",
    "\n",
    "# Stocker dataset sous forme de tableau numpy pour les classes SVM\n",
    "points, labels = X, y\n",
    "\n",
    "# Affichage du nouveau dataset\n",
    "plt.plot(\n",
    "    points[:, 0][labels == -1], points[:, 1][labels == -1], \"bo\", label=\"-1\"\n",
    ")  # Cercles bleus pour -1\n",
    "plt.plot(\n",
    "    points[:, 0][labels == 1], points[:, 1][labels == 1], \"ro\", label=\"+1\"\n",
    ")  # Cercles rouges pour +1\n",
    "plt.title(\"Dataset de 20 points 2D non linéairement séparables\")\n",
    "plt.xlabel(r\"$x_{1}$\")\n",
    "plt.ylabel(r\"$x_{2}$\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ensuite entraîner les différents modèles de SVM sur ce jeu de données et comparer leurs performances avec la même valeur de $C = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des modèles SVM à tester\n",
    "svm_models = {\n",
    "    \"Hinge Loss\": SVMHingeLoss(points, labels, C=1),\n",
    "    \"Ramp Loss\": SVMRampLoss(points, labels, C=1, M=1000),\n",
    "    \"Hard Margin Loss\": SVMHardMarginLoss(points, labels, C=1, M=1000),\n",
    "}\n",
    "\n",
    "# Entraînement des modèles et calcul du taux d'erreur\n",
    "errors = {}\n",
    "\n",
    "for name, model in svm_models.items():\n",
    "    model.train_model()\n",
    "    model.compute_errors()\n",
    "    errors[name] = {\"error rate\" : model.errors[\"error rate\"], \"misclassified points\" : model.errors[\"misclassified points\"]}\n",
    "\n",
    "# Affichage des résultats\n",
    "for name, error in errors.items():\n",
    "    print(\n",
    "        f\"{name}: {error.get('misclassified points')} points mal classés, {error.get('error rate')}% d'erreurs\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le jeu de données généré, nous obtenons les mêmes résultats indépendamment du modèle choisi : 19 points mal classés et 9,5 % d'erreur.\n",
    "\n",
    "Ce résultat peut s'expliquer par les quatre éléments suivants :\n",
    "1. Le dataset est trop simple pour faire apparaître les différences. Les classes étant presque linéairement séparables, les trois modèles trouvent la même frontière optimale : ce choix a été fait pour garantir un temps d'exécution suffisamment faible. Dans ce cas, les erreurs viennent seulement des points dans la marge ou des outliers, donc le taux d'erreur est similaire.\n",
    "\n",
    "2. Dans le cadre de nos modèles, la régularisation L1, qui est la somme des valeurs absolues des poids, force plusieurs coefficients à zéro. Cela peut pousser tous les modèles à converger vers une solution très proche : ce qui est cohérent avec le *dataset* choisi qui ne présente pas de vrai dilemme de classification ;\n",
    "\n",
    "3. La valeur de $C = 1$ utilisée est trop faible. En effet, la pénalisation des erreurs dans *Hinge Loss* et *Ramp Loss* n'est pas très forte.\n",
    "\n",
    "4. La valeur de $M = 1000$ étant très élevée, les variables binaires $z_{i}$ forcent les points avec $\\epsilon_{i} > 1$ à être comptés comme une erreur complète, ce qui peut rendre *Ramp Loss* équivalent à *Hinge Loss* dans certaines conditions.\n",
    "\n",
    "Nous allons pousser plus loin notre analyse en diminuant $M$ pour les modèle *Hinge Loss* *Ramp Loss* puisque pour $M=1000$ cette distinction n'est pas suffisante. Et d'autre part augmenter la valeur de $C$ pour voir si les erreurs des modèles *Hinge Loss* et *Ramp Loss* diminuent contrairement à *Ramp Loss* qui reste stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse quantitative de $M$ sur les erreurs des modèles *Hinge Loss* et *Ramp Loss*\n",
    "\n",
    "Nous allons utiliser les valeurs de $M$ suivantes : $[1, 10, 1000]$ et $C=1$ dans les modèles *Ramp Loss* et *Hard Margin Loss* et comparer leur résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_values = [1, 10, 50, 100, 500]\n",
    "C_value = 1\n",
    "results = {name: [] for name in svm_models.keys()}\n",
    "\n",
    "errors = []\n",
    "\n",
    "for M in M_values:\n",
    "    errors_current_M = {}\n",
    "    for name, model in svm_models.items():\n",
    "        if name in [\"Ramp Loss\", \"Hard Margin Loss\"]:\n",
    "            model = svm_models[name].__class__(points, labels, C=C_value, M=M)\n",
    "        model.train_model()\n",
    "        model.compute_errors()\n",
    "        errors_current_M[name] = model.errors[\"error rate\"]\n",
    "    errors.append(errors_current_M)\n",
    "\n",
    "# Extraire les valeurs des erreurs pour chaque type de perte\n",
    "ramp_loss_errors = [err[\"Ramp Loss\"] for err in errors]\n",
    "hard_margin_errors = [err[\"Hard Margin Loss\"] for err in errors]\n",
    "\n",
    "# Tracer le graphique\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(\n",
    "    M_values,\n",
    "    ramp_loss_errors,\n",
    "    marker=\"o\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Ramp Loss\",\n",
    "    color=\"purple\",\n",
    ")\n",
    "plt.plot(\n",
    "    M_values,\n",
    "    hard_margin_errors,\n",
    "    marker=\"^\",\n",
    "    linestyle=\"-.\",\n",
    "    label=\"Hard Margin Loss\",\n",
    "    color=\"green\",\n",
    ")\n",
    "\n",
    "# Utilisation d'une échelle logarithmique pour M\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Valeur de M (échelle logarithmique)\")\n",
    "plt.ylabel(\"Taux d'erreurs (%)\")\n",
    "plt.title(\"Comparaison des erreurs en fonction de M\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_value = 1000\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "results = {name: [] for name in svm_models.keys()}\n",
    "\n",
    "errors = []\n",
    "\n",
    "for C in C_values:\n",
    "    errors_current_C = {}\n",
    "    for name, model in svm_models.items():\n",
    "        if name in [\"Hinge Loss\"]:\n",
    "            model = svm_models[name].__class__(points, labels, C=C)\n",
    "        elif name in [\"Ramp Loss\", \"Hard Margin Loss\"]:\n",
    "            model = svm_models[name].__class__(points, labels, C=C, M=M_value)\n",
    "        model.train_model()\n",
    "        model.compute_errors()\n",
    "        errors_current_C[name] = model.errors[\"error rate\"]\n",
    "    errors.append(errors_current_C)\n",
    "\n",
    "# Extraire les valeurs des erreurs pour chaque type de perte\n",
    "hinge_loss_errors = [err[\"Hinge Loss\"] for err in errors]\n",
    "ramp_loss_errors = [err[\"Ramp Loss\"] for err in errors]\n",
    "hard_margin_errors = [err[\"Hard Margin Loss\"] for err in errors]\n",
    "\n",
    "# Tracer le graphique\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "plt.plot(\n",
    "    C_values,\n",
    "    hinge_loss_errors,\n",
    "    marker=\"s\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Hinge Loss\",\n",
    "    color=\"orangered\",\n",
    ")\n",
    "plt.plot(\n",
    "    C_values,\n",
    "    ramp_loss_errors,\n",
    "    marker=\"o\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Ramp Loss\",\n",
    "    color=\"purple\",\n",
    ")\n",
    "plt.plot(\n",
    "    C_values,\n",
    "    hard_margin_errors,\n",
    "    marker=\"^\",\n",
    "    linestyle=\"-.\",\n",
    "    label=\"Hard Margin Loss\",\n",
    "    color=\"green\",\n",
    ")\n",
    "\n",
    "# Utilisation d'une échelle logarithmique pour M\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Valeur de C (échelle logarithmique)\")\n",
    "plt.ylabel(\"Taux d'erreurs (%)\")\n",
    "plt.title(\"Comparaison des erreurs en fonction de C (M = 1000)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La figure ci-dessus montre bien qu'à mesure que $C$ augmente, le nombre d'erreurs diminue.\n",
    "Cela est en accord avec les analyses précédentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "L’étude menée sur les différentes variantes des **Machines à Vecteurs de Support (SVM)** met en évidence les forces et les limites des approches utilisées. Le **SVM à marge rigide classique** offre de **très bonnes performances** lorsque les données sont **linéairement séparables**, mais il **échoue** dans le cas contraire, ne trouvant alors **aucune solution**. Pour pallier ce problème, l’introduction de la **régularisation** permet d’assouplir les contraintes et d’obtenir des solutions adaptées aux données **non linéairement séparables**.  \n",
    "\n",
    "L’utilisation de **la hinge loss et de la ramp loss** a également prouvé son efficacité, fournissant des résultats cohérents avec l’objectif de maximisation de la marge tout en gérant les erreurs de classification.  \n",
    "\n",
    "Enfin, l’impact du **paramètre de régularisation $C$** a été analysé pour chaque modèle. Il a été démontré que **plus $C$ est élevé, plus la marge d’erreur diminue**, rendant le modèle plus strict dans sa séparation des classes. Cependant, **une valeur trop élevée de $C$ entraîne un risque de surapprentissage**, compromettant ainsi la généralisation du modèle sur de nouvelles données.  \n",
    "\n",
    "En conclusion, le choix du modèle et de ses hyperparamètres doit être **adapté aux données** afin de garantir un **équilibre optimal entre précision et généralisation**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
